# Parquet as long term storage data format

Parquet was launched and developed by Cloudera and Twitter to serve as a column-based storage format in 2013. It's 
optimized for work with multi-column datasets. You can visit their [official site](https://parquet.apache.org/)

**Parquet files are binary files that contain metadata about their content**. It means without reading/parsing the 
content of the file(s), we can just rely on the metadata to determine column names, compression/encodings, data types 
and even some basic statistics. **The column metadata for a Parquet file is stored at the end of the file, which 
allows for fast, one-pass writing.**

**Parquet is optimized for the Write Once Read Many (WORM) paradigm. It’s slow to write, but incredibly fast to read, 
especially when you’re only accessing a subset of the total columns. Parquet is a good choice for read-heavy workloads. 
For use cases requiring operating on entire rows of data, a format like CSV or AVRO should be used.**

For more details about the parquet data format, please visit [Parquet format explained](https://github.com/pengfei99/ParquetPyArrow/blob/main/docs/Parquet_format_Introduction.md)

## 1. Why we choose parquet?
Normally, when we evaluate a data format, we use the following basic properties. 

- Human Readable
- Compressable
- Splittable
- Complex data structure
- Schema evolution
- Columnar(for better compression and operation performance)
- Framework support

|Property |CSV |Json|Parquet|Avro|ORC|
|---------|----|----|-------|----|---|
|Human Readable|YES|YES|NO|NO|NO|
|Compressable|YES|YES|YES|YES|YES|
|Splittable|YES*|YES*|YES|YES|YES|
|Complex data structure|NO|YES|YES|YES|YES|
|Schema evolution|NO|NO|YES|YES|YES|
|Columnar|NO|NO|YES|NO|YES|

Note:

1. CSV is splittable when it is a raw, uncompressed file or using a splittable compression format such as BZIP2 or LZO (note: LZO needs to be indexed to be splittable!)
2. JSON has the same conditions about splittability when compressed as CSV with one extra difference. When “wholeFile” option is set to true in Spark(re: SPARK-18352), JSON is NOT splittable.

More chocking Number: (todo: add job latency time) 

``` shell
mc ls --summarize s3/pengfei/diffusion/data_format/ny_taxis/parquet/compress/2019_gzip | grep "Total Size"
Total Size: 3.8 GiB
mc ls --summarize s3/pengfei/diffusion/data_format/ny_taxis/csv/2009 | grep "Total Size"
Total Size: 22 GiB
```


### 1.2. Operation latency evaluation for all above data formats 

We have benchmarked all above data formats for the common data operations latency such as:
- read/write
- get basic stats (min, max, avg, count)
- Random data lookup
- Filtering/GroupBy(column-wise)
- Distinct(row-wise)

For more details about the benchmark, [data format overview](https://github.com/pengfei99/data_format_and_optimization/blob/main/notebooks/data_format_overview.ipynb)

After the above analysis, we can say that Orc and Parquet are the best data formats for OLAP applications. They both support various compression algorithms which reduce significantly disk usage. They are both very efficient on columnar-oriented data analysis operations. 

**Parquet has better support on nested data types than Orc**. Orc loses compression ratio and analysis performance when data contains complex nested data types.

**Orc supports data update and ACID (atomicity, consistency, isolation, durability). Parquet does not**, so if you want to update a Parquet file, you need to create a new one based on the old one.

**Parquet has better interoperability** compare to Orc. Because almost all data analysis tools and framework supports parquet. Orc is only supported by Spark, Hive, Impala, MapReduce.

### 1.3 Long-term storage

Parquet is designed for long-term storage and archival purposes, meaning if you write a file today, you can expect that any system that says they can “read Parquet” will be able to read the file in 5 years or 10 years.

### 1.4 Dis-advantage of parquet

- If you need to constantly update(write) your data, do not use columnar-based data formats such as ORC, Parquet. 
- If you need to constantly update your data schema, do not use Parquet. Use Avro.

## 2. Compatibility issues

As parquet is a standard, and is implemented by various frameworks. Some frameworks do not respect or implemented 100% of the parquet standard. This means one parquet file that are generated by one framework may not be readable by another framework. We did not find an official doc which address this issue completely. As a result, the below list may not be complete. So far, we have found four possible compatibility issues:

- Supported compression algorithme 
- Timestamp unity variation (e.g. nanoseconds, microseconds, seconds )
- Unsupported data type
- Metadata specification variation

### 2.1 Supported compression algorithme
Parquet allows us to use various compression algorithme to compress each column. 

Parquet officially supports the following compression algorithms:

- UNCOMPRESSED = 0;
- SNAPPY = 1;
- GZIP = 2;
- LZO = 3;
- BROTLI = 4; (Added in 2.4)
- LZ4 = 5;    (Added in 2.4)
- ZSTD = 6;   (Added in 2.4)

But not all implementation implement all the compression algorithms. For example, pyarrow implements all except **LZO**. Spark by default only includes the **GZIP, and SNAPPY**. For the rest of the algorithme, we need to include the compression codec by ourselves. 

Below shows a benchmark on parquet write time and file size with various framework and compression type.

![Parquet_compression_stats](https://raw.githubusercontent.com/pengfei99/ParquetPyArrow/main/img/parquet_compression_stats.png)

There are other implementation differences. For example, pyarrow allows us to compress each column with a different compression algo. Spark only allows us to specify one compression algo for the entire parquet file when writing a parquet file. Spark can read the mixed compression parquet file without problems as long as the compression algo is supported.

In this notebook [Spark Arrow compression benchmark](https://github.com/pengfei99/ParquetPyArrow/blob/main/notebook/compatibility/SparkArrowCompression.ipynb), we first benchmark the compression latency of pyarrow and spark.
Then we test the compatibility of the output parquet file between pyarrow and spark. 

In this notebook [R Arrow compression benchmark](https://github.com/pengfei99/ParquetPyArrow/blob/main/R/ArrowCompression.Rmd), we first benchmark the compression latency of Rarrow. Then we test the compatibility of the
output parquet file between Rarrow, Pyarrow, spark.


### 2.2 Timestamp implementation variation

#### 2.2.1 Unity variation
Each framework has their own implementation of timestamp, and they may not be compatible. For example, some framework only support timestamps stored in millisecond ('ms') or microsecond ('us') resolution. 

Since pandas uses nanoseconds to represent timestamps, this can occasionally be a nuisance. 
If we use pyarrow to write pandas timestamp in parquet format version 1.0, the nanoseconds must be cast to microseconds (‘us’) manually by using the option **coerce_timestamps**. Otherwise, an error will be raised. After adding the option, it still raise an warning, because we lose time precision.
To suppress this warning, we need to add **allow_truncated_timestamps=True**. Below is a full example

``` python
pq.write_table(table, where, coerce_timestamps='ms', allow_truncated_timestamps=True)
```
If we write in parquet format version 2.0, the nanoseconds are supported, no need to do the conversion.

This notebook [Spark Pyarrow timestamp](https://github.com/pengfei99/ParquetPyArrow/blob/main/notebook/compatibility/ArrowSparkTimeStamp.ipynb)
test the compatibility of timestamp unity between Pyarrow(pandas), Rarrow and spark.

This notebook [Rarrow timestamp](https://github.com/pengfei99/ParquetPyArrow/blob/main/R/ArrowTimeStamp.Rmd) use R to 
read parquet file with timestamp which is generated by spark and Pyarrow.

#### 2.2.2 Timestamp column type

Older Parquet implementations use **INT96** as column type to store the timestamps, but this is now deprecated. Now, the **long** column type is recommended. The **INT96** implementations include some older versions of Apache Impala and Apache Spark. To write timestamps in this format, set the use_deprecated_int96_timestamps option to True in write_table.

``` python
pq.write_table(table, where, use_deprecated_int96_timestamps=True)

```


### 2.3 Unsupported data type

You can get the supported type list from [parquet official website](https://parquet.apache.org/documentation/latest/)
The types are:

- BOOLEAN: 1 bit boolean
- INT32: 32 bit signed ints
- INT64: 64 bit signed ints
- INT96: 96 bit signed ints
- FLOAT: IEEE 32-bit floating point values
- DOUBLE: IEEE 64-bit floating point values
- BYTE_ARRAY: arbitrarily long byte arrays.

#### 2.3.1 Spark does not support unsigned int
Spark(version 2.*) does not support unsigned int as parquet column type. But pyarrow does. So if you write parquet with
unsigned int with pyarrow, spark can't read it. You will get 
```text
org.apache.spark.sql.AnalysisException: Parquet type not supported: INT32 (UINT_16);
```
For more detail, please check 
- [Spark can't read a parquet file created with pyarrow](https://github.com/apache/arrow/issues/1470)
- [parquet-compatibility-with-dask-pandas-and-pyspark](https://stackoverflow.com/questions/59948321/parquet-compatibility-with-dask-pandas-and-pyspark)

This issue has been fixed in Spark 3

### 2.4 Metadata specification variation

#### 2.4.1 Version indicateur
PyArrow uses footer metadata to determine the format version of parquet file, while parquet-mr lib 
(which is used by spark) determines version on the page level by page header type. Moreover, in ParquetFileWriter 
parquet-mr hardcoded version in footer to '1'. 

This should be corrected in spark3. 

For more details, please check [Parquet files v2.0 created by spark can't be read by pyarrow](https://issues.apache.org/jira/browse/ARROW-6057)

## 3. Parquet Optimization