from datetime import datetime, timezone, timedelta

import pandas as pd
from pandas import Timestamp
from pyspark.sql import SparkSession


def main():
    spark = SparkSession.builder \
        .master("local[2]") \
        .appName("PandasSparkTimeStamp") \
        .getOrCreate()

    pdf = pd.DataFrame({'naive': [datetime(2019, 1, 1, 0)],
                        'aware': [Timestamp(year=2019, month=1, day=1,
                                            nanosecond=500, tz=timezone(timedelta(hours=-8)))]})
    # pandas data frame print the datetime
    print(pdf.head())
    # Enable Arrow-based columnar data transfers
    spark.conf.set("spark.sql.execution.arrow.enabled", "true")

    # set up spark session time zone
    spark.conf.set("spark.sql.session.timeZone", "UTC")

    # spark read datetime with UTC timezone
    utc_df = spark.createDataFrame(pdf)
    utc_df.show()

    # spark read datetime with
    spark.conf.set("spark.sql.session.timeZone", "US/Pacific")
    pst_df = spark.createDataFrame(pdf)
    pst_df.show()
    utc_df.show()

    # we convert a spark dataframe back to pandas dataframe
    # as spark does not have time zone, so the generated pandas can't have time zone
    ppst_df = pst_df.toPandas()
    print(ppst_df.head())
    print(ppst_df.info())

    # now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.
    print(ppst_df['aware'][0])
    print(pdf['aware'][0])
    print(f"time zone hours {(ppst_df['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}")


if __name__ == "__main__":
    main()
