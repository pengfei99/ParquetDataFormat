{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Pyarrow compatibility check:  \n",
    "\n",
    "In this section, we will use Pyarrow to Read Parquet file that are generated by spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from pyarrow import fs\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads an arrow table, convert it to a pandas dataframe, then return the shape of the dataframe. \n",
    "def get_shape(table):\n",
    "    df = table.to_pandas()\n",
    "    print(f\"shape of the data set: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write an arrow table to s3 as parquet files\n",
    "# beware when you use the partition cols. For example, if you use date as partition col, and your date has 3000 distinct value, \n",
    "# the output parquet dataset will have 3000 directory, each directory contains one parquet file that has the specific date value.\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "input_path = \"diffusion/data_format/netflix.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'name': 'pengfei/diffusion/data_format/netflix.parquet', 'size': 0, 'type': 'directory'}\n",
      "shape of the data set: (24058262, 3)\n",
      "Arrow read time spents: 7.2179670333862305 s\n"
     ]
    }
   ],
   "source": [
    "# check read time\n",
    "def check_arrow_read_time(endpoint, bucket, path):\n",
    "    t1 = time.time()\n",
    "    arrow_table=read_parquet_from_s3(endpoint, bucket, path)\n",
    "    get_shape(arrow_table)\n",
    "    t2 = time.time()\n",
    "    print(f\"Arrow read time spents: {t2 - t1} s\")\n",
    "    \n",
    "check_arrow_read_time(endpoint,bucket,input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'name': 'pengfei/diffusion/data_format/netflix.parquet', 'size': 0, 'type': 'directory'}\n",
      "user_id\n",
      "rating\n",
      "date\n",
      "Arrow write time spents: 19.849036931991577 s\n"
     ]
    }
   ],
   "source": [
    "# check write time\n",
    "def check_write_time(table, endpoint, bucket_name, path, partition_cols=None):\n",
    "    t1=time.time()\n",
    "    write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols)\n",
    "    t2=time.time()\n",
    "    print(f\"Arrow write time spents: {t2 - t1} s\")\n",
    "\n",
    "arrow_output_path = \"diffusion/data_format/arrow_netflix.parquet\"\n",
    "arrow_table=read_parquet_from_s3(endpoint, bucket, input_path)\n",
    "partition_cols = ['rating']\n",
    "\n",
    "# test write without partition\n",
    "# check_write_time(arrow_table, endpoint, bucket, arrow_output_path)\n",
    "\n",
    "# test write time with partition\n",
    "check_write_time(arrow_table, endpoint, bucket, arrow_output_path, partition_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write an arrow table to s3 as parquet files, you can specify a compression type\n",
    "# compression (str or dict) – Specify the compression codec, either on a general basis or per-column. \n",
    "# Valid values: {‘NONE’, ‘SNAPPY’, ‘GZIP’, ‘BROTLI’, ‘LZ4’, ‘ZSTD’}.\n",
    "# default is snappy.\n",
    "\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY}\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs,compression=compression)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
