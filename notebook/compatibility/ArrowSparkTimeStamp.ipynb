{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **microseconds since the UNIX epoch**.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "## The difference of the timestamp implementation will cause:\n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pandas datetime to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC converted datetime in UTC timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "UTC converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# spark convert the datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "print(\"UTC converted datetime in UTC timezone\")\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# spark convert the datetime with US/Pacific timezone\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "print(\"UTC converted datetime in US/Pacific timezone\")\n",
    "utc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert spark datetime back to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive      aware\n",
      "0 2046-01-01 2046-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   naive   1 non-null      datetime64[ns]\n",
      " 1   aware   1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2)\n",
      "memory usage: 144.0 bytes\n",
      "None\n",
      "2046-01-01 00:00:00\n",
      "2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours -8.0\n"
     ]
    }
   ],
   "source": [
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df = pst_df.toPandas()\n",
    "print(ppst_df.head())\n",
    "print(ppst_df.info())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(ppst_df['aware'][0])\n",
    "print(pdf['aware'][0])\n",
    "print(f\"time zone hours {(ppst_df['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the surprising shift for aware doesn’t happen when the session time zone is UTC (but the timestamps still become “time zone naive”):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 08:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "spark converted to pandas aware time: 2046-01-01 00:00:00\n",
      "pandas aware time: 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the session timezone to UTC again\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "\n",
    "print(f\"spark converted to pandas aware time: {ppst_df['aware'][0]}\")\n",
    "\n",
    "print(f\"pandas aware time: {pdf['aware'][0]}\")\n",
    "\n",
    "(ppst_df['aware'][0].timestamp()-pdf['aware'][0].timestamp())/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod=df.withColumn(\"callDate_unix\",f.unix_timestamp(\"CallDate\",\"dd/MM/yyyy\")) \\\n",
    "   .withColumn(\"callDate_ts\",f.to_timestamp(\"CallDate\",\"dd/MM/yyyy\"))\n",
    "\n",
    "df_mod.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the date compatility of the output parquet\n",
    "\n",
    "In above test, we have test the data conversation via the framework memory converter.\n",
    "\n",
    "Now if we output the date in a parquet file with pyarrow and read it with spark and vise versa. Is it still compatible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "# 1. We creat a pandas data frame and write it in a parquet file\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it as parquet file\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression)\n",
    "    \n",
    "# omit the index by using preserve_index=False\n",
    "table = pa.Table.from_pandas(pdf, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Casting from timestamp[ns, tz=-08:00] to timestamp[us] would lose data: 2398406400000000500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ac5ade66382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pengfei\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"diffusion/data_format/timestamp_compability/arrow_time\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwrite_parquet_as_partitioned_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-716b9604c4d6>\u001b[0m in \u001b[0;36mwrite_parquet_as_partitioned_dataset\u001b[0;34m(table, endpoint, bucket_name, path, partition_cols, compression)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS3FileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'endpoint_url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfile_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{bucket_name}/{path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# omit the index by using preserve_index=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mwrite_to_dataset\u001b[0;34m(table, root_path, partition_cols, partition_filename_cb, filesystem, use_legacy_dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m             write_table(table, f, metadata_collector=metadata_collector,\n\u001b[0;32m-> 1964\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   1965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata_collector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m             \u001b[0mmetadata_collector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, data_page_version, **kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m                 \u001b[0mdata_page_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_page_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 **kwargs) as writer:\n\u001b[0;32m-> 1798\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_group_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_group_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1799\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_group_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_group_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetWriter.write_table\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Casting from timestamp[ns, tz=-08:00] to timestamp[us] would lose data: 2398406400000000500"
     ]
    }
   ],
   "source": [
    "endpoint=os.environ['AWS_S3_ENDPOINT']\n",
    "bucket_name=\"pengfei\"\n",
    "path=\"diffusion/data_format/timestamp_compability/arrow_time\"\n",
    "write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
