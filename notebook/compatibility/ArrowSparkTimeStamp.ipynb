{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **microseconds since the UNIX epoch**.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "## The difference of the timestamp implementation will cause:\n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pandas datetime to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC converted datetime in UTC timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "UTC converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# spark convert the datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "print(\"UTC converted datetime in UTC timezone\")\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# spark convert the datetime with US/Pacific timezone\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "print(\"UTC converted datetime in US/Pacific timezone\")\n",
    "utc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert spark datetime back to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive      aware\n",
      "0 2046-01-01 2046-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   naive   1 non-null      datetime64[ns]\n",
      " 1   aware   1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2)\n",
      "memory usage: 144.0 bytes\n",
      "None\n",
      "2046-01-01 00:00:00\n",
      "2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours -8.0\n"
     ]
    }
   ],
   "source": [
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df = pst_df.toPandas()\n",
    "print(ppst_df.head())\n",
    "print(ppst_df.info())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(ppst_df['aware'][0])\n",
    "print(pdf['aware'][0])\n",
    "print(f\"time zone hours {(ppst_df['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the surprising shift for aware doesn’t happen when the session time zone is UTC (but the timestamps still become “time zone naive”):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 08:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "spark converted to pandas aware time: 2046-01-01 00:00:00\n",
      "pandas aware time: 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the session timezone to UTC again\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "\n",
    "print(f\"spark converted to pandas aware time: {ppst_df['aware'][0]}\")\n",
    "\n",
    "print(f\"pandas aware time: {pdf['aware'][0]}\")\n",
    "\n",
    "(ppst_df['aware'][0].timestamp()-pdf['aware'][0].timestamp())/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod=df.withColumn(\"callDate_unix\",f.unix_timestamp(\"CallDate\",\"dd/MM/yyyy\")) \\\n",
    "   .withColumn(\"callDate_ts\",f.to_timestamp(\"CallDate\",\"dd/MM/yyyy\"))\n",
    "\n",
    "df_mod.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
