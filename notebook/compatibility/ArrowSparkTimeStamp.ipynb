{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **seconds since the UNIX epoch**.\n",
    "2. Note don't mix the long(unix_timestamp) with timestamp(spark_timestamp, microseconds since the unix epoch). They are two different data types.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "In this doc, you can find out all about date and timestamp in spark\n",
    "https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html \n",
    "\n",
    "## The difference of the timestamp implementation will cause: \n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, from_unixtime,lit, unix_timestamp, to_timestamp\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check system timezone\n",
    "First, we need to check the system timezone. Because spark will use it, if we don't specify it. \n",
    "\n",
    "Second, your system may use different timezone system.\n",
    "UTC+1:00 = CET (Central European Time)\n",
    "UTC+2:00 = CEST (Central European Summer Time) \n",
    "\n",
    "CET and CEST represents the same area, countries who use CET switch to CEST during summer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC +0000\n"
     ]
    }
   ],
   "source": [
    "! date +\"%Z %z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. In memory pandas, spark datetime conversion\n",
    "\n",
    "### 1.1 convert pandas dataframe to spark dataframe\n",
    "\n",
    "As spark use session timezone to do the conversion, we need to specify the session timezone before the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC converted datetime in UTC timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "UTC converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# spark convert the datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "print(\"UTC converted datetime in UTC timezone\")\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# spark convert the datetime with US/Pacific timezone\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "print(\"UTC converted datetime in US/Pacific timezone\")\n",
    "utc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert spark dataframe to pandas \n",
    "\n",
    "In the first block, we are in timeZone US/Pacific\n",
    "\n",
    "In the second block, we are in timeZone UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive      aware\n",
      "0 2046-01-01 2046-01-01\n",
      "spark converted pandas data frame 2046-01-01 00:00:00\n",
      "pandas origin data frame2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours -8.0\n"
     ]
    }
   ],
   "source": [
    "# set timezone to US/Pacific\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df1 = pst_df.toPandas()\n",
    "print(ppst_df1.head())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(f\"spark converted pandas data frame {ppst_df1['aware'][0]}\")\n",
    "print(f\"pandas origin data frame{pdf['aware'][0]}\")\n",
    "\n",
    "# the result should be 0, but because spark converted dataframe lost the timezone info, so we have a 8 hour difference. \n",
    "print(f\"time zone hours {(ppst_df1['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the surprising shift for aware does not happen when the session time zone is UTC (but the timestamps still become “time zone naive”):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                naive               aware\n",
      "0 2046-01-01 08:00:00 2046-01-01 08:00:00\n",
      "spark converted pandas data frame 2046-01-01 08:00:00\n",
      "pandas origin data frame2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours 0.0\n"
     ]
    }
   ],
   "source": [
    "# set the session timezone to UTC again\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "ppst_df2 = pst_df.toPandas()\n",
    "print(ppst_df2.head())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(f\"spark converted pandas data frame {ppst_df2['aware'][0]}\")\n",
    "print(f\"pandas origin data frame{pdf['aware'][0]}\")\n",
    "\n",
    "# the result should be 0, but because spark converted dataframe lost the timezone info, so we have a 8 hour difference. \n",
    "print(f\"time zone hours {(ppst_df2['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pandas, spark datetime conversion via Parquet file \n",
    "\n",
    "\n",
    "In above test, we have tested the data conversation via the framework memory converter.\n",
    "\n",
    "Now if we output the date in a parquet file with pyarrow and read it with spark and vise versa. Is it still compatible?\n",
    "\n",
    "## 2.1 Pyarrow pandas parquet file read by spark\n",
    "In this test, we first use pyarrow to write pandas dataframe to a Parquet file with parquet file format version 1.0 and 2.0 (not the arrow framework version). Then we use spark to read these parquet files.\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.1 Write parquet by using pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "# 1. We creat a pandas data frame and write it in a parquet file\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. write it as parquet file\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\",version=\"1.0\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    if version==\"1.0\":\n",
    "        # note without the coerce_timestamps='ms', the write will fail. Because it cant convert the nano second automatically.\n",
    "        # allow_truncated_timestamps=True, suppress the conversion warning (lose time precision)\n",
    "        pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression,version=version, coerce_timestamps='ms', allow_truncated_timestamps=True)\n",
    "    elif version==\"2.0\":\n",
    "        pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression,version=version)\n",
    "    else: \n",
    "        raise ValueError(\"The parquet version must be 1.0 or 2.0\")\n",
    "    \n",
    "# omit the index by using preserve_index=False\n",
    "table = pa.Table.from_pandas(pdf, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrow write to parquet version 1.0. timestamp cast between pandas and arrow lose data\n",
    "# Casting from timestamp[ns, tz=-08:00] to timestamp[us] would lose data: 2398406400000000500\n",
    "# with 2.0, no more warning.\n",
    "\n",
    "endpoint=os.environ['AWS_S3_ENDPOINT']\n",
    "bucket_name=\"pengfei\"\n",
    "path_v1=\"diffusion/data_format/timestamp_compability/arrow_time_v1.0\"\n",
    "path_v2=\"diffusion/data_format/timestamp_compability/arrow_time_v2.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet with format version 1.0\n",
    "write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path_v1,version=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet with format version 2.0\n",
    "write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path_v2,version=\"2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Arrow read it back to pandas df\n",
    "# This function reads a parquet data set (partitioned parquet files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Spark read different parquet file \n",
    "\n",
    "Note, in the arrow implementation of parquet format v1.0. A timestamp in pandas dataframe is converted from nanosecond to microsecond. And in spark, this is considered as **timestamp (spark column type)**\n",
    "\n",
    "Check the schema of the output dataframe of the parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath_v1=f\"s3a://pengfei/{path_v1}\"\n",
    "spath_v2=f\"s3a://pengfei/{path_v2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- naive: timestamp (nullable = true)\n",
      " |-- aware: timestamp (nullable = true)\n",
      " |-- now: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|1633435859|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the compatibility of arrow parquet v1\n",
    "df_v1=spark.read.parquet(spath_v1)\n",
    "df_v1=df_v1.withColumn(\"now\", lit(unix_timestamp()))\n",
    "# you can notice in the dataframe schema, for naive and aware column, they are both recognize as type timestamp automatically\n",
    "# Because in pandas/arrow conversion, we convert the nanosecond to microsecond, which is consider as Spark column type timestamp.  \n",
    "df_v1.printSchema()\n",
    "df_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "df_v1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the arrow implementation of parquet format v2.0. A timestamp in pandas dataframe keeps the nanosecond (long type). When spark read it, spark considered it as long. If we use spark timestamp conversion function such as from_unixtime(). We will have wong result. Because the long column type in spark is considered as second since linux time. To get right timestamp, we need to convert nanosecond to second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- naive: long (nullable = true)\n",
      " |-- aware: long (nullable = true)\n",
      " |-- now: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2398377600000000000|2398406400000000500|1632499078|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the compatibility of arrow parquet v2\n",
    "df_v2=spark.read.parquet(spath_v2)\n",
    "df_v2=df_v2.withColumn(\"now\", lit(unix_timestamp()))\n",
    "df_v2.printSchema()\n",
    "df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+-------------------+\n",
      "|naive_convert         |aware_convert        |now_convert        |\n",
      "+----------------------+---------------------+-------------------+\n",
      "|03-16-+183309 11:28:57|10-10--73164 03:33:37|09-24-2021 16:01:22|\n",
      "+----------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_v2_convert = df_v2.select( \\\n",
    "        from_unixtime(col(\"naive\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"naive_convert\"), \\\n",
    "        from_unixtime(col(\"aware\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"aware_convert\"), \\\n",
    "        from_unixtime(col(\"now\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"now_convert\"))\n",
    "\n",
    "df_v2_convert.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2_nano_to_micro=df_v2.withColumn(\"micro_naive\",col(\"naive\")/1000000000) \\\n",
    "                         .withColumn(\"micro_aware\",col(\"aware\")/1000000000) \\\n",
    "                         .withColumn(\"convert_naive\", from_unixtime(col(\"micro_naive\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_aware\", from_unixtime(col(\"micro_aware\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_now\", from_unixtime(col(\"now\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|      convert_naive|      convert_aware|        convert_now|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|2021-09-24 16:17:41|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2_nano_to_micro.select(\"convert_naive\",\"convert_aware\",\"convert_now\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Spark write parquet with timestamp and use arrow to read it\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html\n",
    "\n",
    "The spark doc says when write to parquet, we can only specify the compression type. There is nothing we can do for time\n",
    "\n",
    "### 2.2.1 Spark write timestamp\n",
    "\n",
    "When spark writes timestamp to parquet file, spark will first check the spark session time zone. If the current session time zone is not UTC, for example US/Pacific, spark will convert the timestamp of the current session time zone (i.e. US/Pacific) to a timestamp of timezone UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"2046-01-01 00:00:00\", \"2046-01-01 08:00:00\")]\n",
    "\n",
    "columns = [\"t1\", \"t2\"]\n",
    "\n",
    "# set a timezone, for the string timestamp, it has zero effect. For the long and timestamp column, it will be converted to the UTC timezone.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "\n",
    "df_v1=spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df_spark_time=df_v1.withColumn(\"t1_long\", unix_timestamp(\"t1\"))\\\n",
    "                   .withColumn(\"t1_timestamp\", to_timestamp(\"t1\")) \\\n",
    "                   .withColumn(\"t2_long\", unix_timestamp(\"t2\"))\\\n",
    "                   .withColumn(\"t2_timestamp\", to_timestamp(\"t2\"))\\\n",
    "                   .select(\"t1\",\"t1_long\",\"t1_timestamp\",\"t2\",\"t2_long\",\"t2_timestamp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t1: string (nullable = true)\n",
      " |-- t1_long: long (nullable = true)\n",
      " |-- t1_timestamp: timestamp (nullable = true)\n",
      " |-- t2: string (nullable = true)\n",
      " |-- t2_long: long (nullable = true)\n",
      " |-- t2_timestamp: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|t1                 |t1_long   |t1_timestamp       |t2                 |t2_long   |t2_timestamp       |\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|2046-01-01 00:00:00|2398406400|2046-01-01 00:00:00|2046-01-01 08:00:00|2398435200|2046-01-01 08:00:00|\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_time.printSchema()\n",
    "df_spark_time.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_parquet_utc_path=\"s3a://pengfei/diffusion/data_format/timestamp_compability/spark_time_v1.0\"\n",
    "spark_parquet_upc_path=\"s3a://pengfei/diffusion/data_format/timestamp_compability/spark_time_upc_v1.0\"\n",
    "\n",
    "# df_spark_time.coalesce(1).write.parquet(spark_parquet_utc_path)\n",
    "# df_spark_time.coalesce(1).write.parquet(spark_parquet_upc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|                 t1|   t1_long|       t1_timestamp|                 t2|   t2_long|       t2_timestamp|\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|2046-01-01 00:00:00|2398377600|2046-01-01 00:00:00|2046-01-01 08:00:00|2398406400|2046-01-01 08:00:00|\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "df_read_utc_parquet=spark.read.parquet(spark_parquet_utc_path)\n",
    "df_read_utc_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|                 t1|   t1_long|       t1_timestamp|                 t2|   t2_long|       t2_timestamp|\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "|2046-01-01 00:00:00|2398406400|2046-01-01 08:00:00|2046-01-01 08:00:00|2398435200|2046-01-01 16:00:00|\n",
      "+-------------------+----------+-------------------+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read_upc_parquet=spark.read.parquet(spark_parquet_upc_path)\n",
    "df_read_upc_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Arrow read the parquet file\n",
    "\n",
    "Arrow read the parquet file, then convert it to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads a parquet data set (partitioned parquet files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'Key': 'pengfei/diffusion/data_format/timestamp_compability/spark_time_v1.0', 'name': 'pengfei/diffusion/data_format/timestamp_compability/spark_time_v1.0', 'type': 'directory', 'Size': 0, 'size': 0, 'StorageClass': 'DIRECTORY'}\n"
     ]
    }
   ],
   "source": [
    "path=\"diffusion/data_format/timestamp_compability/spark_time_v1.0\"\n",
    "\n",
    "pandas_df=read_parquet_from_s3(endpoint, bucket_name, path).to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that all columns of the pandas dataframe has the right column type. You can notice the t1_timestamp and t2_timestamp column have the datetime64 column type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t1_long</th>\n",
       "      <th>t1_timestamp</th>\n",
       "      <th>t2</th>\n",
       "      <th>t2_long</th>\n",
       "      <th>t2_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2046-01-01 00:00:00</td>\n",
       "      <td>2398377600</td>\n",
       "      <td>2046-01-01</td>\n",
       "      <td>2046-01-01 08:00:00</td>\n",
       "      <td>2398406400</td>\n",
       "      <td>2046-01-01 08:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    t1     t1_long t1_timestamp                   t2  \\\n",
       "0  2046-01-01 00:00:00  2398377600   2046-01-01  2046-01-01 08:00:00   \n",
       "\n",
       "      t2_long        t2_timestamp  \n",
       "0  2398406400 2046-01-01 08:00:00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   t1            1 non-null      object        \n",
      " 1   t1_long       1 non-null      int64         \n",
      " 2   t1_timestamp  1 non-null      datetime64[ns]\n",
      " 3   t2            1 non-null      object        \n",
      " 4   t2_long       1 non-null      int64         \n",
      " 5   t2_timestamp  1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(2)\n",
      "memory usage: 176.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the long to pandas timestamp, we can use the function to_datetime. Note the important point is the unit. As spark output timestamp as second, if you use other unit, you will not have correct timestamp. Try to change the unit from s to ns, or ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df['t1_pandas_UNIXTIME'] = pd.to_datetime(pandas_df['t1_long'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t1_long</th>\n",
       "      <th>t1_timestamp</th>\n",
       "      <th>t2</th>\n",
       "      <th>t2_long</th>\n",
       "      <th>t2_timestamp</th>\n",
       "      <th>t1_pandas_UNIXTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2046-01-01 00:00:00</td>\n",
       "      <td>2398377600</td>\n",
       "      <td>2046-01-01</td>\n",
       "      <td>2046-01-01 08:00:00</td>\n",
       "      <td>2398406400</td>\n",
       "      <td>2046-01-01 08:00:00</td>\n",
       "      <td>2046-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    t1     t1_long t1_timestamp                   t2  \\\n",
       "0  2046-01-01 00:00:00  2398377600   2046-01-01  2046-01-01 08:00:00   \n",
       "\n",
       "      t2_long        t2_timestamp t1_pandas_UNIXTIME  \n",
       "0  2398406400 2046-01-01 08:00:00         2046-01-01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   t1                  1 non-null      object        \n",
      " 1   t1_long             1 non-null      int64         \n",
      " 2   t1_timestamp        1 non-null      datetime64[ns]\n",
      " 3   t2                  1 non-null      object        \n",
      " 4   t2_long             1 non-null      int64         \n",
      " 5   t2_timestamp        1 non-null      datetime64[ns]\n",
      " 6   UNIXTIME            1 non-null      datetime64[ns]\n",
      " 7   t1_pandas_UNIXTIME  1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](4), int64(2), object(2)\n",
      "memory usage: 192.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Timestamp String with timezone \n",
    "\n",
    "The best solution to avoid conversion nuance is to use string timestamp with timezone. And for each use case, the data analyste can convert it to numeric timestamp according their needs. \n",
    "\n",
    "## 3.1 Spark Write timestamp to parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+\n",
      "|t1                       |t2                       |\n",
      "+-------------------------+-------------------------+\n",
      "|2046-01-01 00:15:00+01:00|2046-01-01 00:15:00-01:00|\n",
      "+-------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"2046-01-01 00:15:00+01:00\",\"2046-01-01 00:15:00-01:00\")]\n",
    "schema=[\"t1\",\"t2\"]\n",
    "\n",
    "df_tz_raw=spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_tz_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "|t1                       |t2                       |t1_unix   |t2_unix   |t1_ts              |t2_ts              |\n",
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "|2046-01-01 00:15:00+01:00|2046-01-01 00:15:00-01:00|2398374900|2398382100|2045-12-31 23:15:00|2046-01-01 01:15:00|\n",
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set session timezone to system default\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# change session timezone to one that is different to the system timezone. You will see pandas returns a wrong timestamp after \n",
    "# reading the parquet file\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "\n",
    "\n",
    "df_tz=df_tz_raw.withColumn(\"t1_unix\",unix_timestamp(\"t1\",\"yyy-MM-dd HH:mm:ssXXX\")) \\\n",
    "     .withColumn(\"t2_unix\",unix_timestamp(\"t2\",\"yyy-MM-dd HH:mm:ssXXX\")) \\\n",
    "     .withColumn(\"t1_ts\",to_timestamp(\"t1\",\"yyy-MM-dd HH:mm:ssXXX\")) \\\n",
    "     .withColumn(\"t2_ts\",to_timestamp(\"t2\",\"yyy-MM-dd HH:mm:ssXXX\"))\n",
    "\n",
    "df_tz.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_parquet_tz=\"s3a://pengfei/diffusion/data_format/timestamp_compability/spark_timestamp_with_timezone_v1.0\"\n",
    "df_tz.coalesce(1).write.parquet(spark_parquet_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "|t1                       |t2                       |t1_unix   |t2_unix   |t1_ts              |t2_ts              |\n",
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "|2046-01-01 00:15:00+01:00|2046-01-01 00:15:00-01:00|2398374900|2398382100|2045-12-31 23:15:00|2046-01-01 01:15:00|\n",
      "+-------------------------+-------------------------+----------+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read_tz_parquet=spark.read.parquet(spark_parquet_tz)\n",
    "df_read_tz_parquet.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Arrow pandas read parquet with timezone awared timestamp \n",
    "\n",
    "You can notice the result for all three column is correct. Because we write the timestamp by using the system timezone and read it with the same system timezone.\n",
    "\n",
    "If we set the spark session time zone to a timezone that is different to the system timezone, then use arrow to read it with system timezone, you will see a different story.\n",
    "\n",
    "\n",
    "At last, we show how to convert string timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'Key': 'pengfei/diffusion/data_format/timestamp_compability/spark_timestamp_with_timezone_v1.0', 'name': 'pengfei/diffusion/data_format/timestamp_compability/spark_timestamp_with_timezone_v1.0', 'type': 'directory', 'Size': 0, 'size': 0, 'StorageClass': 'DIRECTORY'}\n"
     ]
    }
   ],
   "source": [
    "path=\"diffusion/data_format/timestamp_compability/spark_timestamp_with_timezone_v1.0\"\n",
    "\n",
    "pandas_df=read_parquet_from_s3(endpoint, bucket_name, path).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t1_unix</th>\n",
       "      <th>t2_unix</th>\n",
       "      <th>t1_ts</th>\n",
       "      <th>t2_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2046-01-01 00:15:00+01:00</td>\n",
       "      <td>2046-01-01 00:15:00-01:00</td>\n",
       "      <td>2398374900</td>\n",
       "      <td>2398382100</td>\n",
       "      <td>2045-12-31 23:15:00</td>\n",
       "      <td>2046-01-01 01:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          t1                         t2     t1_unix  \\\n",
       "0  2046-01-01 00:15:00+01:00  2046-01-01 00:15:00-01:00  2398374900   \n",
       "\n",
       "      t2_unix               t1_ts               t2_ts  \n",
       "0  2398382100 2045-12-31 23:15:00 2046-01-01 01:15:00  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 type is : <class 'pandas._libs.tslibs.timestamps.Timestamp'>, t1 value is: 2045-12-31 23:15:00+00:00\n",
      "t2 type is : <class 'pandas._libs.tslibs.timestamps.Timestamp'>, t2 vaule is: 2046-01-01 01:15:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Pandas provides the to_datetime() function which can convert string with time zone to a timezone aware timestamp.\n",
    "t1=pd.to_datetime('2046-01-01 00:15:00+01:00', utc=True)\n",
    "t2=pd.to_datetime('2046-01-01 00:15:00-01:00', utc=True)\n",
    "print(f\"t1 type is : {type(t1)}, t1 value is: {t1}\")\n",
    "print(f\"t2 type is : {type(t2)}, t2 value is: {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}