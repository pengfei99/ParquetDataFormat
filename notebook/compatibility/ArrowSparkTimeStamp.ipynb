{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **seconds since the UNIX epoch**.\n",
    "2. Note don't mix the long(unix_timestamp) with timestamp(spark_timestamp, microseconds since the unix epoch). They are two different data types.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "## The difference of the timestamp implementation will cause: \n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, from_unixtime,lit, unix_timestamp\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pandas datetime to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC converted datetime in UTC timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "UTC converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# spark convert the datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "print(\"UTC converted datetime in UTC timezone\")\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# spark convert the datetime with US/Pacific timezone\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "print(\"UTC converted datetime in US/Pacific timezone\")\n",
    "utc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert spark datetime back to pandas\n",
    "\n",
    "In the first block, we are in timeZone US/Pacific\n",
    "\n",
    "In the second block, we are in timeZone UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive      aware\n",
      "0 2046-01-01 2046-01-01\n",
      "spark converted pandas data frame 2046-01-01 00:00:00\n",
      "pandas origin data frame2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours -8.0\n"
     ]
    }
   ],
   "source": [
    "# set timezone to US/Pacific\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df1 = pst_df.toPandas()\n",
    "print(ppst_df1.head())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(f\"spark converted pandas data frame {ppst_df1['aware'][0]}\")\n",
    "print(f\"pandas origin data frame{pdf['aware'][0]}\")\n",
    "\n",
    "# the result should be 0, but because spark converted dataframe lost the timezone info, so we have a 8 hour difference. \n",
    "print(f\"time zone hours {(ppst_df1['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the surprising shift for aware doesn’t happen when the session time zone is UTC (but the timestamps still become “time zone naive”):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                naive               aware\n",
      "0 2046-01-01 08:00:00 2046-01-01 08:00:00\n",
      "spark converted pandas data frame 2046-01-01 08:00:00\n",
      "pandas origin data frame2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours 0.0\n"
     ]
    }
   ],
   "source": [
    "# set the session timezone to UTC again\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "ppst_df2 = pst_df.toPandas()\n",
    "print(ppst_df2.head())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(f\"spark converted pandas data frame {ppst_df2['aware'][0]}\")\n",
    "print(f\"pandas origin data frame{pdf['aware'][0]}\")\n",
    "\n",
    "# the result should be 0, but because spark converted dataframe lost the timezone info, so we have a 8 hour difference. \n",
    "print(f\"time zone hours {(ppst_df2['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the date compatility of the output parquet\n",
    "\n",
    "In above test, we have test the data conversation via the framework memory converter.\n",
    "\n",
    "Now if we output the date in a parquet file with pyarrow and read it with spark and vise versa. Is it still compatible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "# 1. We creat a pandas data frame and write it in a parquet file\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. write it as parquet file\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\",version=\"1.0\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    if version==\"1.0\":\n",
    "        # note without the coerce_timestamps='ms', the write will fail. Because it cant convert the nano second automatically.\n",
    "        # allow_truncated_timestamps=True, suppress the conversion warning (lose time precision)\n",
    "        pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression,version=version, coerce_timestamps='ms', allow_truncated_timestamps=True)\n",
    "    elif version==\"2.0\":\n",
    "        pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression,version=version)\n",
    "    else: \n",
    "        raise ValueError(\"The parquet version must be 1.0 or 2.0\")\n",
    "    \n",
    "# omit the index by using preserve_index=False\n",
    "table = pa.Table.from_pandas(pdf, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrow write to parquet version 1.0. timestamp cast between pandas and arrow lose data\n",
    "# Casting from timestamp[ns, tz=-08:00] to timestamp[us] would lose data: 2398406400000000500\n",
    "# with 2.0, no more warning.\n",
    "\n",
    "endpoint=os.environ['AWS_S3_ENDPOINT']\n",
    "bucket_name=\"pengfei\"\n",
    "path_v1=\"diffusion/data_format/timestamp_compability/arrow_time_v1.0\"\n",
    "path_v2=\"diffusion/data_format/timestamp_compability/arrow_time_v2.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet with format version 1.0\n",
    "write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path_v1,version=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parquet with format version 2.0\n",
    "write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path_v2,version=\"2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Arrow read it back to pandas df\n",
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spark read different parquet version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath_v1=f\"s3a://pengfei/{path_v1}\"\n",
    "spath_v2=f\"s3a://pengfei/{path_v2}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- naive: timestamp (nullable = true)\n",
      " |-- aware: timestamp (nullable = true)\n",
      " |-- now: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|1632823275|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the compability of arrow parquet v1\n",
    "df_v1=spark.read.parquet(spath_v1)\n",
    "df_v1=df_v1.withColumn(\"now\", lit(unix_timestamp()))\n",
    "# you can notice in the dataframe schema, for naive and aware column, they are both recongnize as type timestamp automatically\n",
    "# Because in pandas/arrow conversion, we convert the nanosecond to microsecond, which is consider as Spart column type timestamp.  \n",
    "df_v1.printSchema()\n",
    "df_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|1632823439|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "df_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- naive: long (nullable = true)\n",
      " |-- aware: long (nullable = true)\n",
      " |-- now: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2398377600000000000|2398406400000000500|1632499078|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the compability of arrow parquet v2\n",
    "df_v2=spark.read.parquet(spath_v2)\n",
    "df_v2=df_v2.withColumn(\"now\", lit(unix_timestamp()))\n",
    "df_v2.printSchema()\n",
    "df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+-------------------+\n",
      "|naive_convert         |aware_convert        |now_convert        |\n",
      "+----------------------+---------------------+-------------------+\n",
      "|03-16-+183309 11:28:57|10-10--73164 03:33:37|09-24-2021 16:01:22|\n",
      "+----------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_v2_convert = df_v2.select( \\\n",
    "        from_unixtime(col(\"naive\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"naive_convert\"), \\\n",
    "        from_unixtime(col(\"aware\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"aware_convert\"), \\\n",
    "        from_unixtime(col(\"now\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"now_convert\"))\n",
    "\n",
    "df_v2_convert.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2_nano_to_micro=df_v2.withColumn(\"micro_naive\",col(\"naive\")/1000000000) \\\n",
    "                         .withColumn(\"micro_aware\",col(\"aware\")/1000000000) \\\n",
    "                         .withColumn(\"convert_naive\", from_unixtime(col(\"micro_naive\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_aware\", from_unixtime(col(\"micro_aware\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_now\", from_unixtime(col(\"now\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|      convert_naive|      convert_aware|        convert_now|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|2021-09-24 16:17:41|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2_nano_to_micro.select(\"convert_naive\",\"convert_aware\",\"convert_now\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
