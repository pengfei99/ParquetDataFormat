{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **microseconds since the UNIX epoch**.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "## The difference of the timestamp implementation will cause:\n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, from_unixtime,lit, unix_timestamp\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pandas datetime to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC converted datetime in UTC timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "UTC converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2045-12-31 16:00:00|2046-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# spark convert the datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "print(\"UTC converted datetime in UTC timezone\")\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "# spark convert the datetime with US/Pacific timezone\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "print(\"UTC converted datetime in US/Pacific timezone\")\n",
    "utc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert spark datetime back to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive      aware\n",
      "0 2046-01-01 2046-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   naive   1 non-null      datetime64[ns]\n",
      " 1   aware   1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2)\n",
      "memory usage: 144.0 bytes\n",
      "None\n",
      "2046-01-01 00:00:00\n",
      "2046-01-01 00:00:00.000000500-08:00\n",
      "time zone hours -8.0\n"
     ]
    }
   ],
   "source": [
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df = pst_df.toPandas()\n",
    "print(ppst_df.head())\n",
    "print(ppst_df.info())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(ppst_df['aware'][0])\n",
    "print(pdf['aware'][0])\n",
    "print(f\"time zone hours {(ppst_df['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the surprising shift for aware doesn’t happen when the session time zone is UTC (but the timestamps still become “time zone naive”):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US/Pacific converted datetime in US/Pacific timezone\n",
      "+-------------------+-------------------+\n",
      "|              naive|              aware|\n",
      "+-------------------+-------------------+\n",
      "|2046-01-01 08:00:00|2046-01-01 08:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "spark converted to pandas aware time: 2046-01-01 00:00:00\n",
      "pandas aware time: 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the session timezone to UTC again\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "print(\"US/Pacific converted datetime in US/Pacific timezone\")\n",
    "pst_df.show()\n",
    "\n",
    "print(f\"spark converted to pandas aware time: {ppst_df['aware'][0]}\")\n",
    "\n",
    "print(f\"pandas aware time: {pdf['aware'][0]}\")\n",
    "\n",
    "(ppst_df['aware'][0].timestamp()-pdf['aware'][0].timestamp())/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7a13e9ac5ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callDate_unix\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CallDate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dd/MM/yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callDate_ts\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CallDate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dd/MM/yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_mod=df.withColumn(\"callDate_unix\",f.unix_timestamp(\"CallDate\",\"dd/MM/yyyy\")) \\\n",
    "   .withColumn(\"callDate_ts\",f.to_timestamp(\"CallDate\",\"dd/MM/yyyy\"))\n",
    "\n",
    "df_mod.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the date compatility of the output parquet\n",
    "\n",
    "In above test, we have test the data conversation via the framework memory converter.\n",
    "\n",
    "Now if we output the date in a parquet file with pyarrow and read it with spark and vise versa. Is it still compatible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       naive                               aware\n",
      "0 2046-01-01 2046-01-01 00:00:00.000000500-08:00\n"
     ]
    }
   ],
   "source": [
    "# 1. We creat a pandas data frame and write it in a parquet file\n",
    "pdf = pd.DataFrame({'naive': [datetime(2046, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2046, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. write it as parquet file\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\",version=\"1.0\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression,version=version)\n",
    "    \n",
    "# omit the index by using preserve_index=False\n",
    "table = pa.Table.from_pandas(pdf, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrow write to parquet version 1.0. timestamp cast between pandas and arrow lose data\n",
    "# Casting from timestamp[ns, tz=-08:00] to timestamp[us] would lose data: 2398406400000000500\n",
    "# with 2.0, no more warning.\n",
    "\n",
    "endpoint=os.environ['AWS_S3_ENDPOINT']\n",
    "bucket_name=\"pengfei\"\n",
    "path_v1=\"diffusion/data_format/timestamp_compability/arrow_time_v1.0\"\n",
    "path_v2=\"diffusion/data_format/timestamp_compability/arrow_time_v2.0\"\n",
    "\n",
    "# write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path_v2,version=\"2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Arrow read it back to pandas df\n",
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spark read different parquet version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath_v1=f\"s3a://pengfei/{path_v1}\"\n",
    "spath_v2=f\"s3a://pengfei/{path_v2}\"\n",
    "# df_v1=spark.read.parquet(spath_v1)\n",
    "df_v2=spark.read.parquet(spath_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- naive: long (nullable = true)\n",
      " |-- aware: long (nullable = true)\n",
      " |-- now: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------+\n",
      "|              naive|              aware|       now|\n",
      "+-------------------+-------------------+----------+\n",
      "|2398377600000000000|2398406400000000500|1632499078|\n",
      "+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2=df_v2.withColumn(\"now\", lit(unix_timestamp()))\n",
    "df_v2.printSchema()\n",
    "df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+-------------------+\n",
      "|naive_convert         |aware_convert        |now_convert        |\n",
      "+----------------------+---------------------+-------------------+\n",
      "|03-16-+183309 11:28:57|10-10--73164 03:33:37|09-24-2021 16:01:22|\n",
      "+----------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_v2_convert = df_v2.select( \\\n",
    "        from_unixtime(col(\"naive\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"naive_convert\"), \\\n",
    "        from_unixtime(col(\"aware\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"aware_convert\"), \\\n",
    "        from_unixtime(col(\"now\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"now_convert\"))\n",
    "\n",
    "df_v2_convert.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2_nano_to_micro=df_v2.withColumn(\"micro_naive\",col(\"naive\")/1000000000) \\\n",
    "                         .withColumn(\"micro_aware\",col(\"aware\")/1000000000) \\\n",
    "                         .withColumn(\"convert_naive\", from_unixtime(col(\"micro_naive\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_aware\", from_unixtime(col(\"micro_aware\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         .withColumn(\"convert_now\", from_unixtime(col(\"now\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|      convert_naive|      convert_aware|        convert_now|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2046-01-01 00:00:00|2046-01-01 08:00:00|2021-09-24 16:17:41|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2_nano_to_micro.select(\"convert_naive\",\"convert_aware\",\"convert_now\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
