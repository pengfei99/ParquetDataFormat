{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Timestamp implementation in different frameworks:\n",
    "\n",
    "**Arrow timestamps** has three parts:\n",
    "1. a **64-bit integer**\n",
    "2. a **metadata** that associates a time unit** (e.g. milliseconds, microseconds, or nanoseconds),\n",
    "3. an **optional time zone**.\n",
    "\n",
    "**Pandas (Timestamp)** has two parts:\n",
    "1. a **64-bit integer** representing **nanoseconds**\n",
    "2. an **optional time zone**.\n",
    "\n",
    "Python/Pandas timestamp types without an associated time zone are referred to as “Time Zone Naive”.\n",
    "Python/Pandas timestamp types with an associated time zone are referred to as “Time Zone Aware”.\n",
    "\n",
    "**Spark timestamps** has one part:\n",
    "1. a **64-bit integers** representing **microseconds since the UNIX epoch**.\n",
    "\n",
    "Note, Spark does not store any metadata about time zones with its timestamps. Spark interprets timestamps with\n",
    "the session local time zone, (i.e. spark.sql.session.timeZone). If that time zone is undefined, Spark turns to\n",
    "the default system time zone.\n",
    "\n",
    "## The difference of the timestamp implementation will cause:\n",
    "\n",
    "- Timezone information is lost (all timestamps that result from converting from spark to arrow/pandas are “time zone naive”).\n",
    "\n",
    "- Timestamps are truncated to microseconds.\n",
    "\n",
    "- The session time zone might have unintuitive impacts on translation of timestamp values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert pandas datetime to spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"PandasSparkTimeStamp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.DataFrame({'naive': [datetime(2019, 1, 1, 0)],\n",
    "                    'aware': [Timestamp(year=2019, month=1, day=1,\n",
    "                                        nanosecond=500, tz=timezone(timedelta(hours=-8)))]})\n",
    "# pandas data frame print the datetime\n",
    "print(pdf.head())\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# set up spark session time zone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# spark read datetime with UTC timezone\n",
    "utc_df = spark.createDataFrame(pdf)\n",
    "utc_df.show()\n",
    "\n",
    "# if we change the spark session time zone, and read datetime with it.\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\")\n",
    "pst_df = spark.createDataFrame(pdf)\n",
    "pst_df.show()\n",
    "utc_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we convert a spark dataframe back to pandas dataframe\n",
    "# as spark does not have time zone, so the generated pandas can't have time zone\n",
    "ppst_df = pst_df.toPandas()\n",
    "print(ppst_df.head())\n",
    "print(ppst_df.info())\n",
    "\n",
    "# now we compare the datetime of origin pandas dataframe with the dataframe generated by spark.\n",
    "print(ppst_df['aware'][0])\n",
    "print(pdf['aware'][0])\n",
    "print(f\"time zone hours {(ppst_df['aware'][0].timestamp() - pdf['aware'][0].timestamp()) / 3600}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}