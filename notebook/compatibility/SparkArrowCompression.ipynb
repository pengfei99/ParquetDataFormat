{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Arrow compression test \n",
    "In this section, we use spark and arrow to output parquet files with different compression algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "# spark.rpc.message.maxSize if for write large csv file. The default value is 128, here we set it to 1024\n",
    "if local:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"local[4]\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .getOrCreate()\n",
    "else: \n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0923 14:15:29.814286    2508 request.go:655] Throttling request took 1.173250271s, request: GET:https://kubernetes.default/apis/coordination.k8s.io/v1beta1?timeout=32s\n",
      "NAME                                            READY   STATUS    RESTARTS   AGE\n",
      "flume-test-agent-df8c5b944-vtjbx                1/1     Running   0          4d\n",
      "jupyter-324928-7b4cdf67dd-tk99l                 1/1     Running   0          5h28m\n",
      "kafka-server-0                                  1/1     Running   0          4d1h\n",
      "kafka-server-1                                  1/1     Running   0          4d\n",
      "kafka-server-2                                  1/1     Running   0          4d2h\n",
      "kafka-server-zookeeper-0                        1/1     Running   0          4d\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-1   0/1     Error     0          5h18m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-2   0/1     Error     0          5h18m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-3   0/1     Error     0          5h18m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-4   0/1     Error     0          5h18m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/ny_taxis/parquet/raw\"\n",
    "compress_output_path = \"s3a://pengfei/diffusion/data_format/ny_taxis/parquet/compress\"\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\"\n",
    "csv_input_path=\"s3a://pengfei/diffusion/data_format/ny_taxis/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 170896055 rows, 18 columns\n",
      "Spark read above data frame in parquet format, and spents: 20.538439750671387 s\n",
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_at: timestamp (nullable = true)\n",
      " |-- dropoff_at: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- rate_code_id: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_spark_parquet_read_time(path:str)->DataFrame:\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read above data frame in parquet format, and spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# read parquet generated by arrow    \n",
    "df=check_spark_parquet_read_time(parquet_input_path)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# read parquet generated by spark\n",
    "# check_spark_parquet_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_csv_write_time(df:DataFrame,path:str):\n",
    "    t1=time.time()\n",
    "    df.coalesce(1).write.option(\"header\",\"true\").csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "\n",
    "# check_spark_csv_write_time(df,f\"{csv_input_path}/2011_2012\")   \n",
    "\n",
    "def check_spark_csv_read_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# df_fire=check_spark_read_csv_time(csv_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_parquet_write_time(df,path,partition_number,compression_algo):\n",
    "    t1=time.time()\n",
    "    df.coalesce(partition_number).write \\\n",
    "    .option(\"parquet.compression\",compression_algo) \\\n",
    "    .parquet(path) \n",
    "    t2=time.time()\n",
    "    print(f\"Spark write parquet with {compression_algo} compression, it spents : {t2 - t1} s\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spark compression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Spark Compress with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with gzip compression, it spents : 327.7600781917572 s\n"
     ]
    }
   ],
   "source": [
    "# Spark write parquet with gzip compression, it spents : 327.7600781917572 s\n",
    "\n",
    "comp_algo=\"gzip\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 3.8 GiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/ny_taxis/parquet/compress/2009_gzip | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Spark compress with snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with snappy compression, it spents : 210.80829095840454 s\n"
     ]
    }
   ],
   "source": [
    "#Spark write parquet with snappy compression, it spents : 210.80829095840454 s\n",
    "\n",
    "comp_algo=\"snappy\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 4.5 GiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/ny_taxis/parquet/compress/2009_snappy | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Spark Compress with lz4\n",
    "missing lz4 dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lz4\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Spark compress with lzo\n",
    "missing lzo dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lzo\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Spark compress with brotli\n",
    "doc says it's not supported by default, so missing brotli dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"brotli\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Spark compress with zstd\n",
    "doc says it's not supported by default, so missing zstd dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# zstd is not supported by default\n",
    "comp_algo=\"zstd\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2019_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyarrow writes parquet with compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table\n",
    "\n",
    "# check read time\n",
    "def check_arrow_read_time(endpoint, bucket, path):\n",
    "    t1 = time.time()\n",
    "    arrow_table=read_parquet_from_s3(endpoint, bucket, path)\n",
    "    get_shape(arrow_table)\n",
    "    t2 = time.time()\n",
    "    print(f\"Arrow read time spents: {t2 - t1} s\")\n",
    "    return arrow_table\n",
    "    \n",
    "# This function reads an arrow table, convert it to a pandas dataframe, then return the shape of the dataframe. \n",
    "def get_shape(table):\n",
    "    df = table.to_pandas()\n",
    "    print(f\"shape of the data set: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "# don't add / after raw, it will raise error\n",
    "input_path = \"diffusion/data_format/Fire_Department.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'name': 'pengfei/diffusion/data_format/Fire_Department.parquet', 'size': 0, 'type': 'directory'}\n",
      "shape of the data set: (5500520, 35)\n",
      "Arrow read time spents: 37.68216300010681 s\n"
     ]
    }
   ],
   "source": [
    "arrow_table=check_arrow_read_time(endpoint,bucket, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           _c0      _c1              _c2        _c3         _c4         _c5  \\\n",
      "0  Call Number  Unit ID  Incident Number  Call Type   Call Date  Watch Date   \n",
      "1    210391607      E19         21017645     Alarms  02/08/2021  02/08/2021   \n",
      "\n",
      "                      _c6                     _c7                     _c8  \\\n",
      "0           Received DtTm              Entry DtTm           Dispatch DtTm   \n",
      "1  02/08/2021 01:00:14 PM  02/08/2021 01:01:36 PM  02/08/2021 01:01:40 PM   \n",
      "\n",
      "                      _c9  ...             _c25              _c26       _c27  \\\n",
      "0           Response DtTm  ...  Call Type Group  Number of Alarms  Unit Type   \n",
      "1  02/08/2021 01:03:21 PM  ...            Alarm                 1     ENGINE   \n",
      "\n",
      "                             _c28                      _c29  \\\n",
      "0  Unit sequence in call dispatch  Fire Prevention District   \n",
      "1                               1                         8   \n",
      "\n",
      "                  _c30                                  _c31           _c32  \\\n",
      "0  Supervisor District  Neighborhooods - Analysis Boundaries          RowID   \n",
      "1                    7                             Lakeshore  210391607-E19   \n",
      "\n",
      "                                           _c33                    _c34  \n",
      "0                                 case_location  Analysis Neighborhoods  \n",
      "1  POINT (-122.48045074945836 37.7190118676788)                      16  \n",
      "\n",
      "[2 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "pdf=arrow_table.to_pandas()\n",
    "print(pdf.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write an arrow table to s3 as parquet files, you can specify a compression type\n",
    "# compression (str or dict) – Specify the compression codec, either on a general basis or per-column. \n",
    "# Valid values: {‘NONE’, ‘SNAPPY’, ‘GZIP’, ‘BROTLI’, ‘LZ4’, ‘ZSTD’}.\n",
    "# default is snappy.\n",
    "\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression)\n",
    "    \n",
    "# check write time\n",
    "def check_write_time(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    t1=time.time()\n",
    "    write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols,compression=compression)\n",
    "    t2=time.time()\n",
    "    print(f\"Arrow write time spents: {t2 - t1} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 40.83500552177429 s\n"
     ]
    }
   ],
   "source": [
    "output_path=f\"diffusion/data_format/arrow_snappy_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 619 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_snappy_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 31.99910068511963 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_zstd_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"ZSTD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 393 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_zstd_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 152.42934775352478 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_gzip_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 380 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_gzip_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 41.850053787231445 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_lz4_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"LZ4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 612 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_lz4_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 176.08239126205444 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_brotli_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"BROTLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 340 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_brotli_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression and use dictionary encoding by column\n",
    "arrow allows us to specify compression and dictionary codec per column\n",
    "spark does not, spark can only specify a global compression and dicionary codec.\n",
    "\n",
    "pq.write_table(table, where, compression={'foo': 'snappy', 'bar': 'gzip'}, use_dictionary=['foo', 'bar'])\n",
    "\n",
    "# to test if we have a mix compression, spark can read correctly or not.\n",
    "\n",
    "Write a mixed compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://{endpoint}\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "file_uri = f\"{bucket_name}/{path}\"\n",
    "pq.write_table(table, root_path=file_uri, filesystem=fs, compression={:\"SNAPPY\",:\"GZIP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 5500520 rows, 35 columns\n",
      "Spark read above data frame in parquet format, and spents: 2.1240062713623047 s\n"
     ]
    }
   ],
   "source": [
    "path1=\"s3a://pengfei/diffusion/data_format/arrow_snappy_fire_department.parquet\"\n",
    "path2=\"s3a://pengfei/diffusion/data_format/Fire_Department.parquet\"\n",
    "df=check_spark_parquet_read_time(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
