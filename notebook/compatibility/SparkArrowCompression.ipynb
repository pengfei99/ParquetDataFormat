{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Arrow compression test \n",
    "In this section, we use spark and arrow to output parquet files with different compression algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "# spark.rpc.message.maxSize if for write large csv file. The default value is 128, here we set it to 1024\n",
    "if local:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"local[4]\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .getOrCreate()\n",
    "else: \n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0923 16:56:25.213237    3014 request.go:655] Throttling request took 1.170551256s, request: GET:https://kubernetes.default/apis/cert-manager.io/v1?timeout=32s\n",
      "NAME                                            READY   STATUS    RESTARTS   AGE\n",
      "flume-test-agent-df8c5b944-vtjbx                1/1     Running   0          4d3h\n",
      "jupyter-324928-7b4cdf67dd-tk99l                 1/1     Running   0          8h\n",
      "kafka-server-0                                  1/1     Running   0          4d4h\n",
      "kafka-server-1                                  1/1     Running   0          4d3h\n",
      "kafka-server-2                                  1/1     Running   0          4d4h\n",
      "kafka-server-zookeeper-0                        1/1     Running   0          4d3h\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-1   0/1     Error     0          7h59m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-2   0/1     Error     0          7h59m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-3   0/1     Error     0          7h59m\n",
      "sparkarrowcompression-03c6b67c11df10d9-exec-4   0/1     Error     0          7h59m\n",
      "sparkarrowcompression-cec7447c13033895-exec-1   1/1     Running   0          160m\n",
      "sparkarrowcompression-cec7447c13033895-exec-2   1/1     Running   0          160m\n",
      "sparkarrowcompression-cec7447c13033895-exec-3   1/1     Running   0          160m\n",
      "sparkarrowcompression-cec7447c13033895-exec-4   1/1     Running   0          160m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/sf_fire/parquet/spark_sf_fire_gzip\"\n",
    "compress_output_path = \"s3a://pengfei/diffusion/data_format/sf_fire/parquet\"\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\"\n",
    "csv_input_path=\"s3a://pengfei/diffusion/data_format/ny_taxis/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+------------+----+-----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+-------------------+--------------------+--------------+--------------------+\n",
      "|CallNumber| UnitID|IncidentNumber|            CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|       TransportDtTm|        HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|         City|ZipcodeofIncident|Battalion| StationArea| Box| OriginalPriority|Priority|FinalPriority|ALSUnit|       CallTypeGroup|NumberofAlarms|      UnitType|Unitsequenceincalldispatch|FirePreventionDistrict| SupervisorDistrict|NeighborhoodDistrict|      Location|               RowID|\n",
      "+----------+-------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+------------+----+-----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+-------------------+--------------------+--------------+--------------------+\n",
      "|      null|Unit ID|          null|           Call Type| Call Date|Watch Date|       Received DtTm|          Entry DtTm|       Dispatch DtTm|       Response DtTm|       On Scene DtTm|      Transport DtTm|       Hospital DtTm|Call Final Dispos...|      Available DtTm|             Address|         City|             null|Battalion|Station Area| Box|Original Priority|Priority|         null|   null|     Call Type Group|          null|     Unit Type|                      null|  Fire Prevention D...|Supervisor District|Neighborhooods - ...|         RowID|       case_location|\n",
      "| 210391607|    E19|      21017645|              Alarms|02/08/2021|02/08/2021|02/08/2021 01:00:...|02/08/2021 01:01:...|02/08/2021 01:01:...|02/08/2021 01:03:...|02/08/2021 01:05:...|                null|                null|                Fire|02/08/2021 01:18:...|400 Block of SERR...|San Francisco|            94132|      B08|          19|8581|                3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     8|                  7|           Lakeshore| 210391607-E19|POINT (-122.48045...|\n",
      "| 210391164|    T04|      21017596|              Alarms|02/08/2021|02/08/2021|02/08/2021 10:54:...|02/08/2021 10:56:...|02/08/2021 10:56:...|02/08/2021 10:57:...|02/08/2021 10:59:...|                null|                null|                Fire|02/08/2021 11:06:...|600 Block of LONG...|San Francisco|            94158|      B03|          04|2264|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         1|                     3|                  6|         Mission Bay| 210391164-T04|POINT (-122.39227...|\n",
      "| 210391034|    E16|      21017578|Citizen Assist / ...|02/08/2021|02/08/2021|02/08/2021 10:18:...|02/08/2021 10:19:...|02/08/2021 10:19:...|02/08/2021 10:20:...|02/08/2021 10:27:...|                null|                null|                Fire|02/08/2021 10:53:...|FRANKLIN ST/FILBE...|San Francisco|            94123|      B04|          16|3233|                3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     4|                  2|              Marina| 210391034-E16|POINT (-122.42581...|\n",
      "| 210390767|    T19|      21017552|               Other|02/08/2021|02/08/2021|02/08/2021 08:50:...|02/08/2021 08:54:...|02/08/2021 08:55:...|02/08/2021 08:57:...|                null|                null|                null|                Fire|02/08/2021 09:02:...|CALL BOX: JOHN DA...|    Daly City|             null|      B09|          33|9922|                3|       3|            3|   true|                Fire|             1|         TRUCK|                         9|                  None|               None|                None| 210390767-T19|POINT (-122.46239...|\n",
      "| 210382984|    B05|      21017398|              Alarms|02/07/2021|02/07/2021|02/07/2021 09:18:...|02/07/2021 09:20:...|02/07/2021 09:21:...|02/07/2021 09:21:...|02/07/2021 09:25:...|                null|                null|                Fire|02/07/2021 09:35:...|2100 Block of FEL...|San Francisco|            94117|      B05|          12|4554|                3|       3|            3|  false|               Alarm|             1|         CHIEF|                         2|                     5|                  5|   Lone Mountain/USF| 210382984-B05|POINT (-122.45328...|\n",
      "| 210382403|    T05|      21017307|              Alarms|02/07/2021|02/07/2021|02/07/2021 05:24:...|02/07/2021 05:26:...|02/07/2021 05:26:...|02/07/2021 05:27:...|02/07/2021 05:30:...|                null|                null|                Fire|02/07/2021 05:50:...|1400 Block of GEA...|San Francisco|            94109|      B04|          38|3323|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                     4|                  5|           Japantown| 210382403-T05|POINT (-122.42636...|\n",
      "| 210381978|    B10|      21017263|        Outside Fire|02/07/2021|02/07/2021|02/07/2021 03:09:...|02/07/2021 03:09:...|02/07/2021 03:11:...|02/07/2021 03:11:...|02/07/2021 03:11:...|                null|                null|                Fire|02/07/2021 03:39:...|500 Block of AMAD...|San Francisco|            94124|      B10|          25|6463|                3|       3|            3|  false|                Fire|             1|         CHIEF|                         5|                  10.0|                 10|Bayview Hunters P...| 210381978-B10|POINT (-122.38299...|\n",
      "| 210381601|    T06|      21017206|              Alarms|02/07/2021|02/07/2021|02/07/2021 01:07:...|02/07/2021 01:09:...|02/07/2021 01:09:...|02/07/2021 01:10:...|02/07/2021 01:11:...|                null|                null|                Fire|02/07/2021 01:16:...|0 Block of SANCHE...|San Francisco|            94114|      B05|          06|5131|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                     2|                  8| Castro/Upper Market| 210381601-T06|POINT (-122.43120...|\n",
      "| 210381356|   KM12|      21017173|    Medical Incident|02/07/2021|02/07/2021|02/07/2021 12:00:...|02/07/2021 12:00:...|02/07/2021 12:01:...|02/07/2021 12:02:...|02/07/2021 12:12:...|02/07/2021 12:27:...|02/07/2021 12:33:...|    Code 2 Transport|02/07/2021 01:05:...|2300 Block of 16T...|San Francisco|            94103|      B02|          29|5241|                2|       2|            2|  false|Non Life-threatening|             1|       PRIVATE|                         3|                     2|                 10|             Mission|210381356-KM12|POINT (-122.40952...|\n",
      "| 210380503|    E10|      21017065|              Alarms|02/07/2021|02/06/2021|02/07/2021 05:41:...|02/07/2021 05:42:...|02/07/2021 05:42:...|02/07/2021 05:44:...|02/07/2021 05:46:...|                null|                null|                Fire|02/07/2021 06:15:...|1400 Block of BAK...|San Francisco|            94115|      B05|          10|4262|                3|       3|            3|   true|               Alarm|             1|        ENGINE|                         2|                     5|                  5|           Japantown| 210380503-E10|POINT (-122.44301...|\n",
      "| 210380252|    E19|      21017021|              Alarms|02/07/2021|02/06/2021|02/07/2021 02:11:...|02/07/2021 02:12:...|02/07/2021 02:12:...|02/07/2021 02:14:...|02/07/2021 02:17:...|                null|                null|                Fire|02/07/2021 02:24:...|300 Block of ARBA...|San Francisco|            94132|      B08|          19|8582|                3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     8|                  7|           Lakeshore| 210380252-E19|POINT (-122.48306...|\n",
      "| 210372754|    E08|      21016911|              Alarms|02/06/2021|02/06/2021|02/06/2021 06:50:...|02/06/2021 06:51:...|02/06/2021 06:52:...|02/06/2021 06:53:...|02/06/2021 06:56:...|                null|                null|                Fire|02/06/2021 06:59:...|   0 Block of 6TH ST|San Francisco|            94103|      B03|          01|2251|                3|       3|            3|   true|               Alarm|             1|        ENGINE|                         2|                     3|                  6|     South of Market| 210372754-E08|POINT (-122.40902...|\n",
      "| 210370471|    B06|      21016646|      Structure Fire|02/06/2021|02/05/2021|02/06/2021 06:44:...|02/06/2021 06:46:...|02/06/2021 06:46:...|02/06/2021 06:48:...|02/06/2021 06:57:...|                null|                null|                Fire|02/06/2021 06:57:...|0 Block of DAKOTA ST|San Francisco|            94107|      B10|          37|2614|                3|       3|            3|  false|               Alarm|             1|         CHIEF|                         4|                    10|                 10|        Potrero Hill| 210370471-B06|POINT (-122.39563...|\n",
      "| 210370398|    T07|      21016635|              Alarms|02/06/2021|02/05/2021|02/06/2021 05:33:...|02/06/2021 05:33:...|02/06/2021 05:33:...|02/06/2021 05:36:...|02/06/2021 05:40:...|                null|                null|                Fire|02/06/2021 05:42:...|800 Block of POTR...|San Francisco|            94110|      B10|          07|2553|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                    10|                 10|        Potrero Hill| 210370398-T07|POINT (-122.40672...|\n",
      "| 210370261|    T12|      21016610|              Alarms|02/06/2021|02/05/2021|02/06/2021 03:24:...|02/06/2021 03:27:...|02/06/2021 03:27:...|02/06/2021 03:29:...|02/06/2021 03:36:...|                null|                null|                Fire|02/06/2021 03:40:...|200 Block of UPPE...|San Francisco|            94117|      B05|          12|5165|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         3|                     5|                  8|      Haight Ashbury| 210370261-T12|POINT (-122.44517...|\n",
      "| 210362631|    T17|      21016433|              Alarms|02/05/2021|02/05/2021|02/05/2021 05:04:...|02/05/2021 05:06:...|02/05/2021 05:06:...|                null|                null|                null|                null|                Fire|02/05/2021 05:10:...|5800 Block of 3RD ST|San Francisco|            94124|      B10|          17|6537|                3|       3|            3|  false|               Alarm|             1|         TRUCK|                         3|                  10.0|                 10|Bayview Hunters P...| 210362631-T17|POINT (-122.39472...|\n",
      "| 210360668|    E17|      21016151|        Outside Fire|02/05/2021|02/05/2021|02/05/2021 08:01:...|02/05/2021 08:05:...|02/05/2021 08:10:...|02/05/2021 08:10:...|02/05/2021 08:13:...|                null|                null|                Fire|02/05/2021 08:15:...|600 Block of INNE...|Hunters Point|            94124|      B10|          17|6663|                3|       3|            3|   true|                Fire|             1|        ENGINE|                         1|                    10|                 10|Bayview Hunters P...| 210360668-E17|POINT (-122.37111...|\n",
      "| 210360399|     77|      21016122|    Medical Incident|02/05/2021|02/04/2021|02/05/2021 05:22:...|02/05/2021 05:24:...|02/05/2021 05:24:...|02/05/2021 05:25:...|                null|                null|                null|    Code 2 Transport|02/05/2021 05:28:...| 100 Block of 6TH ST|San Francisco|            94103|      B03|          01|2251|                3|       3|            3|   true|Potentially Life-...|             1|         MEDIC|                         3|                     3|                  6|     South of Market|  210360399-77|POINT (-122.40848...|\n",
      "| 210363081|    RC1|      21016489|    Medical Incident|02/05/2021|02/05/2021|02/05/2021 07:29:...|02/05/2021 07:29:...|02/05/2021 07:33:...|02/05/2021 07:34:...|                null|                null|                null|    Code 2 Transport|02/05/2021 07:35:...|  MISSION ST/10TH ST|San Francisco|            94103|      B02|          36|2341|                3|       E|            3|   true|Potentially Life-...|             1|RESCUE CAPTAIN|                         3|                     2|                  6|     South of Market| 210363081-RC1|POINT (-122.41590...|\n",
      "+----------+-------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+------------+----+-----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+-------------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "['CallNumber', 'UnitID', 'IncidentNumber', 'CallType', 'CallDate', 'WatchDate', 'ReceivedDtTm', 'EntryDtTm', 'DispatchDtTm', 'ResponseDtTm', 'OnSceneDtTm', 'TransportDtTm', 'HospitalDtTm', 'CallFinalDisposition', 'AvailableDtTm', 'Address', 'City', 'ZipcodeofIncident', 'Battalion', 'StationArea', 'Box', 'OriginalPriority', 'Priority', 'FinalPriority', 'ALSUnit', 'CallTypeGroup', 'NumberofAlarms', 'UnitType', 'Unitsequenceincalldispatch', 'FirePreventionDistrict', 'SupervisorDistrict', 'NeighborhoodDistrict', 'Location', 'RowID']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 170896055 rows, 18 columns\n",
      "Spark read above data frame in parquet format, and spents: 7.731244802474976 s\n",
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_at: timestamp (nullable = true)\n",
      " |-- dropoff_at: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- rate_code_id: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_spark_parquet_read_time(path:str)->DataFrame:\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read above data frame in parquet format, and spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# read parquet generated by arrow    \n",
    "df=check_spark_parquet_read_time(parquet_input_path)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# read parquet generated by spark\n",
    "# check_spark_parquet_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_csv_write_time(df:DataFrame,path:str):\n",
    "    t1=time.time()\n",
    "    df.coalesce(1).write.option(\"header\",\"true\").csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "\n",
    "# check_spark_csv_write_time(df,f\"{csv_input_path}/2011_2012\")   \n",
    "\n",
    "def check_spark_csv_read_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# df_fire=check_spark_read_csv_time(csv_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_parquet_write_time(df,path,partition_number,compression_algo):\n",
    "    t1=time.time()\n",
    "    df.coalesce(partition_number).write \\\n",
    "    .option(\"parquet.compression\",compression_algo) \\\n",
    "    .parquet(path) \n",
    "    t2=time.time()\n",
    "    print(f\"Spark write parquet with {compression_algo} compression, it spents : {t2 - t1} s\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spark compression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Spark Compress with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with gzip compression, it spents : 72.66732335090637 s\n"
     ]
    }
   ],
   "source": [
    "# Spark write parquet with gzip compression ny 2009, it spents : 327.7600781917572 s\n",
    "\n",
    "comp_algo=\"gzip\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/spark_sf_fire_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 355 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/spark_sf_fire_gzip | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Spark compress with snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with snappy compression, it spents : 212.33866047859192 s\n"
     ]
    }
   ],
   "source": [
    "#Spark write parquet with snappy compression ny 2009, it spents : 210.80829095840454 s\n",
    "\n",
    "comp_algo=\"snappy\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/spark_sf_fire_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 4.5 GiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/spark_sf_fire_snappy | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Spark Compress with lz4\n",
    "missing lz4 dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lz4\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Spark compress with lzo\n",
    "missing lzo dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lzo\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Spark compress with brotli\n",
    "doc says it's not supported by default, so missing brotli dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"brotli\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Spark compress with zstd\n",
    "doc says it's not supported by default, so missing zstd dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# zstd is not supported by default\n",
    "comp_algo=\"zstd\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2019_{comp_algo}\",8,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyarrow writes parquet with compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table\n",
    "\n",
    "# check read time\n",
    "def check_arrow_read_time(endpoint, bucket, path):\n",
    "    t1 = time.time()\n",
    "    arrow_table=read_parquet_from_s3(endpoint, bucket, path)\n",
    "    get_shape(arrow_table)\n",
    "    t2 = time.time()\n",
    "    print(f\"Arrow read time spents: {t2 - t1} s\")\n",
    "    return arrow_table\n",
    "    \n",
    "# This function reads an arrow table, convert it to a pandas dataframe, then return the shape of the dataframe. \n",
    "def get_shape(table):\n",
    "    df = table.to_pandas()\n",
    "    print(f\"shape of the data set: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "# don't add / after raw, it will raise error\n",
    "input_path = \"diffusion/data_format/sf_fire/parquet/spark_sf_fire_snappy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'name': 'pengfei/diffusion/data_format/Fire_Department.parquet', 'size': 0, 'type': 'directory'}\n",
      "shape of the data set: (5500520, 35)\n",
      "Arrow read time spents: 37.68216300010681 s\n"
     ]
    }
   ],
   "source": [
    "arrow_table=check_arrow_read_time(endpoint,bucket, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           _c0      _c1              _c2        _c3         _c4         _c5  \\\n",
      "0  Call Number  Unit ID  Incident Number  Call Type   Call Date  Watch Date   \n",
      "1    210391607      E19         21017645     Alarms  02/08/2021  02/08/2021   \n",
      "\n",
      "                      _c6                     _c7                     _c8  \\\n",
      "0           Received DtTm              Entry DtTm           Dispatch DtTm   \n",
      "1  02/08/2021 01:00:14 PM  02/08/2021 01:01:36 PM  02/08/2021 01:01:40 PM   \n",
      "\n",
      "                      _c9  ...             _c25              _c26       _c27  \\\n",
      "0           Response DtTm  ...  Call Type Group  Number of Alarms  Unit Type   \n",
      "1  02/08/2021 01:03:21 PM  ...            Alarm                 1     ENGINE   \n",
      "\n",
      "                             _c28                      _c29  \\\n",
      "0  Unit sequence in call dispatch  Fire Prevention District   \n",
      "1                               1                         8   \n",
      "\n",
      "                  _c30                                  _c31           _c32  \\\n",
      "0  Supervisor District  Neighborhooods - Analysis Boundaries          RowID   \n",
      "1                    7                             Lakeshore  210391607-E19   \n",
      "\n",
      "                                           _c33                    _c34  \n",
      "0                                 case_location  Analysis Neighborhoods  \n",
      "1  POINT (-122.48045074945836 37.7190118676788)                      16  \n",
      "\n",
      "[2 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "pdf=arrow_table.to_pandas()\n",
    "print(pdf.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write an arrow table to s3 as parquet files, you can specify a compression type\n",
    "# compression (str or dict) – Specify the compression codec, either on a general basis or per-column. \n",
    "# Valid values: {‘NONE’, ‘SNAPPY’, ‘GZIP’, ‘BROTLI’, ‘LZ4’, ‘ZSTD’}.\n",
    "# default is snappy.\n",
    "\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression)\n",
    "    \n",
    "# check write time\n",
    "def check_write_time(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    t1=time.time()\n",
    "    write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols,compression=compression)\n",
    "    t2=time.time()\n",
    "    print(f\"Arrow write time spents: {t2 - t1} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 40.83500552177429 s\n"
     ]
    }
   ],
   "source": [
    "output_path=f\"diffusion/data_format/arrow_snappy_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 619 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_snappy_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 31.99910068511963 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_zstd_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"ZSTD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 393 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_zstd_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 152.42934775352478 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_gzip_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 380 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_gzip_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 41.850053787231445 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_lz4_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"LZ4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 612 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_lz4_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 176.08239126205444 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/arrow_brotli_fire_department.parquet\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"BROTLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 340 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/arrow_brotli_fire_department.parquet | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression and use dictionary encoding by column\n",
    "arrow allows us to specify compression and dictionary codec per column\n",
    "spark does not, spark can only specify a global compression and dicionary codec.\n",
    "\n",
    "pq.write_table(table, where, compression={'foo': 'snappy', 'bar': 'gzip'}, use_dictionary=['foo', 'bar'])\n",
    "\n",
    "# to test if we have a mix compression, spark can read correctly or not.\n",
    "\n",
    "Write a mixed compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://{endpoint}\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "file_uri = f\"{bucket_name}/{path}\"\n",
    "pq.write_table(table, root_path=file_uri, filesystem=fs, compression={:\"SNAPPY\",:\"GZIP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 5500520 rows, 35 columns\n",
      "Spark read above data frame in parquet format, and spents: 2.1240062713623047 s\n"
     ]
    }
   ],
   "source": [
    "path1=\"s3a://pengfei/diffusion/data_format/arrow_snappy_fire_department.parquet\"\n",
    "path2=\"s3a://pengfei/diffusion/data_format/Fire_Department.parquet\"\n",
    "df=check_spark_parquet_read_time(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
