{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Arrow compression test \n",
    "In this section, we use spark and arrow to output parquet files with different compression algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "# spark.rpc.message.maxSize if for write large csv file. The default value is 128, here we set it to 1024\n",
    "if local:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"local[4]\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .getOrCreate()\n",
    "else: \n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0924 09:19:59.144597    1725 request.go:655] Throttling request took 1.164686495s, request: GET:https://kubernetes.default/apis/authorization.k8s.io/v1?timeout=32s\n",
      "NAME                               READY   STATUS    RESTARTS   AGE\n",
      "flume-test-agent-df8c5b944-vtjbx   1/1     Running   0          4d19h\n",
      "jupyter-324928-7b4cdf67dd-tk99l    1/1     Running   0          24h\n",
      "jupyter-542018-6d9c59bc68-vrzh4    1/1     Running   0          97m\n",
      "kafka-server-0                     1/1     Running   0          4d20h\n",
      "kafka-server-1                     1/1     Running   0          4d19h\n",
      "kafka-server-2                     1/1     Running   0          4d21h\n",
      "kafka-server-zookeeper-0           1/1     Running   0          4d19h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0924 07:57:12.365932     685 request.go:655] Throttling request took 1.177662055s, request: GET:https://kubernetes.default/apis/apiextensions.k8s.io/v1?timeout=32s\n",
      "I0924 07:57:15.715832     719 request.go:655] Throttling request took 1.179433162s, request: GET:https://kubernetes.default/apis/networking.k8s.io/v1beta1?timeout=32s\n",
      "pod \"sparkarrowcompression-03c6b67c11df10d9-exec-1\" deleted\n",
      "pod \"sparkarrowcompression-03c6b67c11df10d9-exec-2\" deleted\n",
      "pod \"sparkarrowcompression-03c6b67c11df10d9-exec-3\" deleted\n",
      "pod \"sparkarrowcompression-03c6b67c11df10d9-exec-4\" deleted\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods | grep Error | awk '{print $1}' | xargs kubectl delete pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/sf_fire/parquet/raw\"\n",
    "compress_output_path = \"s3a://pengfei/diffusion/data_format/sf_fire/parquet\"\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\"\n",
    "csv_input_path=\"s3a://pengfei/diffusion/data_format/ny_taxis/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CallNumber', 'UnitID', 'IncidentNumber', 'CallType', 'CallDate', 'WatchDate', 'ReceivedDtTm', 'EntryDtTm', 'DispatchDtTm', 'ResponseDtTm', 'OnSceneDtTm', 'TransportDtTm', 'HospitalDtTm', 'CallFinalDisposition', 'AvailableDtTm', 'Address', 'City', 'ZipcodeofIncident', 'Battalion', 'StationArea', 'Box', 'OriginalPriority', 'Priority', 'FinalPriority', 'ALSUnit', 'CallTypeGroup', 'NumberofAlarms', 'UnitType', 'Unitsequenceincalldispatch', 'FirePreventionDistrict', 'SupervisorDistrict', 'NeighborhoodDistrict', 'Location', 'RowID']\n",
      "+----------+------+--------------+--------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+--------------------+-------------+--------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|TransportDtTm|HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|         City|ZipcodeofIncident|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumberofAlarms|UnitType|Unitsequenceincalldispatch|FirePreventionDistrict|SupervisorDistrict|NeighborhoodDistrict|     Location|               RowID|\n",
      "+----------+------+--------------+--------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+--------------------+-------------+--------------------+\n",
      "| 210391607|   E19|      21017645|  Alarms|02/08/2021|02/08/2021|02/08/2021 01:00:...|02/08/2021 01:01:...|02/08/2021 01:01:...|02/08/2021 01:03:...|02/08/2021 01:05:...|         null|        null|                Fire|02/08/2021 01:18:...|400 Block of SERR...|San Francisco|            94132|      B08|         19|8581|               3|       3|            3|   true|        Alarm|             1|  ENGINE|                         1|                     8|                 7|           Lakeshore|210391607-E19|POINT (-122.48045...|\n",
      "+----------+------+--------------+--------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+-------------+--------------+--------+--------------------------+----------------------+------------------+--------------------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- ReceivedDtTm: string (nullable = true)\n",
      " |-- EntryDtTm: string (nullable = true)\n",
      " |-- DispatchDtTm: string (nullable = true)\n",
      " |-- ResponseDtTm: string (nullable = true)\n",
      " |-- OnSceneDtTm: string (nullable = true)\n",
      " |-- TransportDtTm: string (nullable = true)\n",
      " |-- HospitalDtTm: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZipcodeofIncident: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- Unitsequenceincalldispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- NeighborhoodDistrict: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireSchema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                     StructField('UnitID', StringType(), True),\n",
    "                     StructField('IncidentNumber', IntegerType(), True),\n",
    "                     StructField('CallType', StringType(), True),                  \n",
    "                     StructField('CallDate', StringType(), True),       \n",
    "                     StructField('WatchDate', StringType(), True),       \n",
    "                     StructField('ReceivedDtTm', StringType(), True),       \n",
    "                     StructField('EntryDtTm', StringType(), True),       \n",
    "                     StructField('DispatchDtTm', StringType(), True),       \n",
    "                     StructField('ResponseDtTm', StringType(), True),       \n",
    "                     StructField('OnSceneDtTm', StringType(), True),       \n",
    "                     StructField('TransportDtTm', StringType(), True),                  \n",
    "                     StructField('HospitalDtTm', StringType(), True),       \n",
    "                     StructField('CallFinalDisposition', StringType(), True),       \n",
    "                     StructField('AvailableDtTm', StringType(), True),       \n",
    "                     StructField('Address', StringType(), True),       \n",
    "                     StructField('City', StringType(), True),       \n",
    "                     StructField('ZipcodeofIncident', IntegerType(), True),       \n",
    "                     StructField('Battalion', StringType(), True),                 \n",
    "                     StructField('StationArea', StringType(), True),       \n",
    "                     StructField('Box', StringType(), True),       \n",
    "                     StructField('OriginalPriority', StringType(), True),       \n",
    "                     StructField('Priority', StringType(), True),       \n",
    "                     StructField('FinalPriority', IntegerType(), True),       \n",
    "                     StructField('ALSUnit', BooleanType(), True),       \n",
    "                     StructField('CallTypeGroup', StringType(), True),\n",
    "                     StructField('NumberofAlarms', IntegerType(), True),\n",
    "                     StructField('UnitType', StringType(), True),\n",
    "                     StructField('Unitsequenceincalldispatch', IntegerType(), True),\n",
    "                     StructField('FirePreventionDistrict', StringType(), True),\n",
    "                     StructField('SupervisorDistrict', StringType(), True),\n",
    "                     StructField('NeighborhoodDistrict', StringType(), True),\n",
    "                     StructField('Location', StringType(), True),\n",
    "                     StructField('RowID', StringType(), True)])\n",
    "data_path=\"s3a://pengfei/diffusion/data_format/Fire_Department_Calls_for_Service.csv\"\n",
    "df = spark.read.options(delimiter=',').option(\"header\",\"true\").schema(fireSchema).csv(data_path)\n",
    "print(df.columns)\n",
    "df.show(1)\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 5500520 rows, 34 columns\n",
      "Spark read above data frame in parquet format, and spents: 1.9366462230682373 s\n",
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- ReceivedDtTm: string (nullable = true)\n",
      " |-- EntryDtTm: string (nullable = true)\n",
      " |-- DispatchDtTm: string (nullable = true)\n",
      " |-- ResponseDtTm: string (nullable = true)\n",
      " |-- OnSceneDtTm: string (nullable = true)\n",
      " |-- TransportDtTm: string (nullable = true)\n",
      " |-- HospitalDtTm: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZipcodeofIncident: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- Unitsequenceincalldispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- NeighborhoodDistrict: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_spark_parquet_read_time(path:str)->DataFrame:\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read above data frame in parquet format, and spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# read parquet generated by arrow    \n",
    "df=check_spark_parquet_read_time(parquet_input_path)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# read parquet generated by spark\n",
    "# check_spark_parquet_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_csv_write_time(df:DataFrame,path:str):\n",
    "    t1=time.time()\n",
    "    df.coalesce(1).write.option(\"header\",\"true\").csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "\n",
    "# check_spark_csv_write_time(df,f\"{csv_input_path}/2011_2012\")   \n",
    "\n",
    "def check_spark_csv_read_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# df_fire=check_spark_read_csv_time(csv_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_parquet_write_time(df,path,partition_number,compression_algo):\n",
    "    t1=time.time()\n",
    "    df.coalesce(partition_number).write \\\n",
    "    .option(\"parquet.compression\",compression_algo) \\\n",
    "    .parquet(path) \n",
    "    t2=time.time()\n",
    "    print(f\"Spark write parquet with {compression_algo} compression, it spents : {t2 - t1} s\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spark compression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Spark Compress with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with gzip compression, it spents : 74.42315220832825 s\n"
     ]
    }
   ],
   "source": [
    "# Spark write parquet with gzip compression ny 2009, it spents : 327.7600781917572 s\n",
    "\n",
    "comp_algo=\"gzip\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/spark_sf_fire_{comp_algo}\",4,comp_algo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 355 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/spark_sf_fire_gzip | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Spark compress with snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write parquet with snappy compression, it spents : 50.436259031295776 s\n"
     ]
    }
   ],
   "source": [
    "#Spark write parquet with snappy compression ny 2009, it spents : 210.80829095840454 s\n",
    "\n",
    "comp_algo=\"snappy\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/spark_sf_fire_{comp_algo}\",4,comp_algo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 552 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/spark_sf_fire_snappy | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Spark Compress with lz4\n",
    "missing lz4 dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lz4\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Spark compress with lzo\n",
    "missing lzo dependencies, but the doc says it's supported by default \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"lzo\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Spark compress with brotli\n",
    "doc says it's not supported by default, so missing brotli dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_algo=\"brotli\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2009_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Spark compress with zstd\n",
    "doc says it's not supported by default, so missing zstd dependencies is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# zstd is not supported by default\n",
    "comp_algo=\"zstd\"\n",
    "check_spark_parquet_write_time(df,f\"{compress_output_path}/2019_{comp_algo}\",4,comp_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyarrow writes parquet with compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function reads a parquet data set (partitioned partque files) from s3, and returns an arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs, metadata_nthreads=8)\n",
    "    table = dataset.read()\n",
    "    return table\n",
    "\n",
    "# check read time\n",
    "def check_arrow_read_time(endpoint, bucket, path):\n",
    "    t1 = time.time()\n",
    "    arrow_table=read_parquet_from_s3(endpoint, bucket, path)\n",
    "    get_shape(arrow_table)\n",
    "    t2 = time.time()\n",
    "    print(f\"Arrow read time spents: {t2 - t1} s\")\n",
    "    return arrow_table\n",
    "    \n",
    "# This function reads an arrow table, convert it to a pandas dataframe, then return the shape of the dataframe. \n",
    "def get_shape(table):\n",
    "    df = table.to_pandas()\n",
    "    print(f\"shape of the data set: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "# don't add / after raw, it will raise error\n",
    "input_path = \"diffusion/data_format/sf_fire/parquet/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file metadata: {'Key': 'pengfei/diffusion/data_format/sf_fire/parquet/raw', 'name': 'pengfei/diffusion/data_format/sf_fire/parquet/raw', 'type': 'directory', 'Size': 0, 'size': 0, 'StorageClass': 'DIRECTORY'}\n",
      "shape of the data set: (5500519, 34)\n",
      "Arrow read time spents: 27.993977308273315 s\n"
     ]
    }
   ],
   "source": [
    "arrow_table=check_arrow_read_time(endpoint,bucket, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CallNumber UnitID  IncidentNumber CallType    CallDate   WatchDate  \\\n",
      "0   210391607    E19        21017645   Alarms  02/08/2021  02/08/2021   \n",
      "1   210391164    T04        21017596   Alarms  02/08/2021  02/08/2021   \n",
      "\n",
      "             ReceivedDtTm               EntryDtTm            DispatchDtTm  \\\n",
      "0  02/08/2021 01:00:14 PM  02/08/2021 01:01:36 PM  02/08/2021 01:01:40 PM   \n",
      "1  02/08/2021 10:54:56 AM  02/08/2021 10:56:50 AM  02/08/2021 10:56:57 AM   \n",
      "\n",
      "             ResponseDtTm  ... ALSUnit CallTypeGroup NumberofAlarms UnitType  \\\n",
      "0  02/08/2021 01:03:21 PM  ...    True         Alarm              1   ENGINE   \n",
      "1  02/08/2021 10:57:07 AM  ...   False         Alarm              1    TRUCK   \n",
      "\n",
      "  Unitsequenceincalldispatch FirePreventionDistrict SupervisorDistrict  \\\n",
      "0                        1.0                      8                  7   \n",
      "1                        1.0                      3                  6   \n",
      "\n",
      "   NeighborhoodDistrict       Location  \\\n",
      "0             Lakeshore  210391607-E19   \n",
      "1           Mission Bay  210391164-T04   \n",
      "\n",
      "                                           RowID  \n",
      "0   POINT (-122.48045074945836 37.7190118676788)  \n",
      "1  POINT (-122.39227179213904 37.77288298280324)  \n",
      "\n",
      "[2 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "pdf=arrow_table.to_pandas()\n",
    "print(pdf.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write an arrow table to s3 as parquet files, you can specify a compression type\n",
    "# compression (str or dict) – Specify the compression codec, either on a general basis or per-column. \n",
    "# Valid values: {‘NONE’, ‘SNAPPY’, ‘GZIP’, ‘BROTLI’, ‘LZ4’, ‘ZSTD’}.\n",
    "# default is snappy.\n",
    "\n",
    "def write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, partition_cols=partition_cols, filesystem=fs, compression=compression)\n",
    "    \n",
    "# check write time\n",
    "def check_write_time(table, endpoint, bucket_name, path, partition_cols=None, compression=\"SNAPPY\"):\n",
    "    t1=time.time()\n",
    "    write_parquet_as_partitioned_dataset(table, endpoint, bucket_name, path, partition_cols,compression=compression)\n",
    "    t2=time.time()\n",
    "    print(f\"Arrow write time spents: {t2 - t1} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 37.47578525543213 s\n"
     ]
    }
   ],
   "source": [
    "output_path=f\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_snappy\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 596 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_snappy | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 29.63837957382202 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_zstd\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"ZSTD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 385 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_zstd | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 138.10498523712158 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_gzip\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 372 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_gzip | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 38.89105200767517 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_lz4\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"LZ4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 589 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_lz4 | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 163.5519528388977 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_brotli\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"BROTLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 332 MiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_brotli | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow write time spents: 72.52444410324097 s\n"
     ]
    }
   ],
   "source": [
    "output_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_none\"\n",
    "check_write_time(arrow_table,endpoint,bucket,output_path,compression=\"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 1.3 GiB\n"
     ]
    }
   ],
   "source": [
    "! mc ls --summarize s3/pengfei/diffusion/data_format/sf_fire/parquet/arrow_sf_fire_none | grep \"Total Size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression and use dictionary encoding by column\n",
    "arrow allows us to specify compression and dictionary codec per column\n",
    "spark does not, spark can only specify a global compression and dicionary codec.\n",
    "\n",
    "pq.write_table(table, where, compression={'foo': 'snappy', 'bar': 'gzip'}, use_dictionary=['foo', 'bar'])\n",
    "\n",
    "# to test if we have a mix compression, spark can read correctly or not.\n",
    "\n",
    "Write a mixed compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write_table() missing 1 required positional argument: 'where'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-00c151c76eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"diffusion/data_format/sf_fire/parquet/arrow_mix_compression\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{bucket}/{path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"UnitID\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"SNAPPY\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NumberofAlarms\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"GZIP\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: write_table() missing 1 required positional argument: 'where'"
     ]
    }
   ],
   "source": [
    "url = f\"https://{endpoint}\"\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "path=\"diffusion/data_format/sf_fire/parquet/arrow_mix_compression\"\n",
    "file_uri = f\"{bucket}/{path}\"\n",
    "pq.write_table(arrow_table, file_uri, filesystem=fs, compression={\"UnitID\":\"SNAPPY\",\"NumberofAlarms\":\"GZIP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 5500520 rows, 35 columns\n",
      "Spark read above data frame in parquet format, and spents: 2.1240062713623047 s\n"
     ]
    }
   ],
   "source": [
    "path1=\"s3a://pengfei/diffusion/data_format/arrow_snappy_fire_department.parquet\"\n",
    "path2=\"s3a://pengfei/diffusion/data_format/Fire_Department.parquet\"\n",
    "df=check_spark_parquet_read_time(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-836580d537b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
