{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark compatibility check\n",
    "In this section, we use spark to read parquet file that are generated by arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkReadArrow\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0915 14:22:37.084522    2169 request.go:655] Throttling request took 1.168941502s, request: GET:https://kubernetes.default/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s\n",
      "NAME                                                        READY   STATUS      RESTARTS   AGE\n",
      "argo-workflows-315671-server-7d5cf97d7-rft67                1/1     Running     0          2d7h\n",
      "argo-workflows-315671-workflow-controller-f9545ff99-nmzlj   1/1     Running     0          2d7h\n",
      "flume-test-agent-df8c5b944-j47lw                            1/1     Running     0          3d17h\n",
      "jupyter-107079-85887dc86c-4h46d                             1/1     Running     0          6h13m\n",
      "kafka-server-0                                              1/1     Running     0          3d19h\n",
      "kafka-server-1                                              1/1     Running     0          3d16h\n",
      "kafka-server-2                                              1/1     Running     0          4d17h\n",
      "kafka-server-zookeeper-0                                    1/1     Running     0          3d19h\n",
      "mlflow-752700-849d994c7c-ktcw2                              1/1     Running     0          47h\n",
      "mlflow-db-0                                                 1/1     Running     0          47h\n",
      "openfood-workflow-v1-6b7dx-2499703885                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-2894184417                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-3450937515                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-3747732804                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-987440259                        0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-d654v-101709803                        0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-1307372657                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-1413982577                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-2057086870                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-3251949423                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-fzsqf-1791550650                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-2177260728                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-4190006776                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-4240129049                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-671590606                        0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-rfhq6-1051279974                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-2311733954                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-3222782558                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-3944585314                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-727072442                        0/2     Completed   0          2d\n",
      "openfood-workflow-v1-v6n9j-1360484915                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-1705025569                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-2796457920                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-367709877                        0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-382658779                        0/2     Completed   0          2d1h\n",
      "postgres-1616502799-67f86f5bdf-2n5nl                        1/1     Running     0          2d5h\n",
      "sparkreadarrow-c8aa427be97763d6-exec-1                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-2                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-3                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-4                      1/1     Running     0          103m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-1        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-2        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-3        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-4        1/1     Running     0          151m\n",
      "ubuntu-555591-684684bcc7-nw79c                              1/1     Running     0          3d17h\n",
      "vscode-120409-cb9cd6cd7-f57hb                               1/1     Running     0          3d5h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/arrow_netflix/\"\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 24058262 rows, 3 columns\n",
      "Spark read time spents: 0.8141007423400879 s\n",
      "data frame has 24058262 rows, 3 columns\n",
      "Spark read time spents: 0.8090753555297852 s\n"
     ]
    }
   ],
   "source": [
    "def check_spark_read_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "# read parquet generated by arrow    \n",
    "check_spark_read_time(parquet_input_path)\n",
    "\n",
    "# read parquet generated by spark\n",
    "check_spark_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write time spents: 26.110023498535156 s\n"
     ]
    }
   ],
   "source": [
    "def check_spark_write_time(df,path):\n",
    "    t1=time.time()\n",
    "    df.write.parquet(path)\n",
    "    t2=time.time()\n",
    "    print(f\"Spark write time spents: {t2 - t1} s\")\n",
    "    \n",
    "df=spark.read.parquet(parquet_input_path)\n",
    "check_spark_write_time(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|double|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     2|     4|\n",
      "|     3|     9|\n",
      "|     4|    16|\n",
      "|     5|    25|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|single|triple|\n",
      "+------+------+\n",
      "|     6|   216|\n",
      "|     7|   343|\n",
      "|     8|   512|\n",
      "|     9|   729|\n",
      "|    10|  1000|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-651d046d3c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_output_path1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_output_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1 already exists."
     ]
    }
   ],
   "source": [
    "# merge schema example\n",
    "# In this example, we create two data frame, \n",
    "# df1, we have two columns: single, double.\n",
    "# df2, we have two columns: single, triple\n",
    "sc = spark.sparkContext\n",
    "schema_output_path1= \"s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1\"\n",
    "schema_output_path2= \"s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=2\"\n",
    "df1 = spark.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "df2.show()\n",
    "# then we write the two data frame in a partition folder, here we put key=1, key=2.\n",
    "df1.write.parquet(schema_output_path1)\n",
    "df2.write.parquet(schema_output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+---+\n",
      "|single|double|key|\n",
      "+------+------+---+\n",
      "|     9|  null|  2|\n",
      "|    10|  null|  2|\n",
      "|     4|    16|  1|\n",
      "|     5|    25|  1|\n",
      "|     6|  null|  2|\n",
      "|     7|  null|  2|\n",
      "|     8|  null|  2|\n",
      "|     1|     1|  1|\n",
      "|     2|     4|  1|\n",
      "|     3|     9|  1|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we read the parent folder that contains the partitioned folder. The partition key become a column name, we call it partitioned column\n",
    "parent_path=\"s3a://pengfei/diffusion/data_format/merge_schema/test_table\"\n",
    "# as the data frame in each partition folder has different schema, we need to set mergeSchema to true. Otherwise it will only use the schema\n",
    "# of the first parquet file which it reads. \n",
    "# set the below value to false to check the output data frame.\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(parent_path)\n",
    "mergedDF.printSchema()\n",
    "\n",
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
