{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark compatibility check\n",
    "In this section, we use spark to read parquet file that are generated by arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "# rpc.message.maxSize if for write large csv file. The default value is 128, here we set it to 1024\n",
    "if local:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"local[4]\") \\\n",
    "    .appName(\"SparkReadArrow\") \\\n",
    "    .getOrCreate()\n",
    "else: \n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkReadArrow\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0922 07:39:05.655316    8944 request.go:655] Throttling request took 1.173939039s, request: GET:https://kubernetes.default/apis/crd.projectcalico.org/v1?timeout=32s\n",
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "flume-test-agent-df8c5b944-vtjbx         1/1     Running   0          2d18h\n",
      "jupyter-12907-54b4894749-5h87t           1/1     Running   0          23h\n",
      "jupyter-168161-5cbfcc8f85-5qn9h          1/1     Running   0          28m\n",
      "kafka-server-0                           1/1     Running   0          2d18h\n",
      "kafka-server-1                           1/1     Running   0          2d18h\n",
      "kafka-server-2                           1/1     Running   0          2d19h\n",
      "kafka-server-zookeeper-0                 1/1     Running   0          2d18h\n",
      "sparkreadarrow-5491277c0c714586-exec-1   1/1     Running   0          24s\n",
      "sparkreadarrow-5491277c0c714586-exec-2   1/1     Running   0          24s\n",
      "sparkreadarrow-5491277c0c714586-exec-3   1/1     Running   0          24s\n",
      "sparkreadarrow-5491277c0c714586-exec-4   1/1     Running   0          24s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/ny_taxis/parquet/raw_2011_2012\"\n",
    "\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\"\n",
    "csv_input_path=\"s3a://pengfei/diffusion/data_format/ny_taxis/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 355441523 rows, 18 columns\n",
      "Spark read above data frame in parquet format, and spents: 15.679622888565063 s\n",
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_at: timestamp (nullable = true)\n",
      " |-- dropoff_at: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- rate_code_id: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_spark_parquet_read_time(path:str)->DataFrame:\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read above data frame in parquet format, and spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# read parquet generated by arrow    \n",
    "df=check_spark_parquet_read_time(parquet_input_path)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# read parquet generated by spark\n",
    "# check_spark_parquet_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spark_csv_write_time(df:DataFrame,path:str):\n",
    "    t1=time.time()\n",
    "    df.coalesce(1).write.option(\"header\",\"true\").csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "\n",
    "check_spark_csv_write_time(df,f\"{csv_input_path}/2011_2012\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write time spents: 26.110023498535156 s\n"
     ]
    }
   ],
   "source": [
    "def check_spark_write_time(df,path):\n",
    "    t1=time.time()\n",
    "    df.write.parquet(path)\n",
    "    t2=time.time()\n",
    "    print(f\"Spark write time spents: {t2 - t1} s\")\n",
    "    \n",
    "df=spark.read.parquet(parquet_input_path)\n",
    "check_spark_write_time(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|          pickup_at|         dropoff_at|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|      VTS|2010-01-26 07:41:00|2010-01-26 07:45:00|              1|         0.75|       -73.95678|       40.76775|           1|              null|        -73.96596|       40.765232|         CAS|        4.5|  0.0|    0.5|       0.0|         0.0|         5.0|\n",
      "|      DDS|2010-01-30 23:31:00|2010-01-30 23:46:12|              1|          5.9|       -73.99612|       40.76393|           1|              null|       -73.981514|        40.74119|         CAS|       15.3|  0.5|    0.5|       0.0|         0.0|        16.3|\n",
      "|      DDS|2010-01-18 20:22:20|2010-01-18 20:38:12|              1|          4.0|      -73.979675|       40.78379|           1|              null|       -73.917854|        40.87856|         CAS|       11.7|  0.5|    0.5|       0.0|         0.0|        12.7|\n",
      "|      VTS|2010-01-09 01:18:00|2010-01-09 01:35:00|              2|          4.7|       -73.97792|      40.763996|           1|              null|       -73.923904|       40.759724|         CAS|       13.3|  0.5|    0.5|       0.0|         0.0|        14.3|\n",
      "|      CMT|2010-01-18 19:10:14|2010-01-18 19:17:07|              1|          0.6|       -73.99092|       40.73468|           1|                 0|       -73.995514|       40.739086|         Cre|        5.3|  0.0|    0.5|      0.87|         0.0|        6.67|\n",
      "|      DDS|2010-01-23 18:40:25|2010-01-23 18:54:51|              1|          3.3|             0.0|            0.0|           1|              null|              0.0|             0.0|         CRE|       10.5|  0.0|    0.5|       1.0|         0.0|        12.0|\n",
      "|      VTS|2010-01-17 09:18:00|2010-01-17 09:25:00|              1|         1.33|      -73.993744|      40.754917|           1|              null|        -73.98472|       40.755928|         CAS|        6.1|  0.0|    0.5|       0.0|         0.0|         6.6|\n",
      "|      VTS|2010-01-09 13:49:00|2010-01-09 13:56:00|              1|         1.83|       -73.97103|       40.75131|           1|              null|        -73.99056|       40.734924|         CAS|        6.9|  0.0|    0.5|       0.0|         0.0|         7.4|\n",
      "|      VTS|2010-01-09 00:25:00|2010-01-09 00:39:00|              1|         3.28|      -73.990036|       40.72563|           1|              null|        -73.99384|       40.761696|         CAS|       11.3|  0.5|    0.5|       0.0|         0.0|        12.3|\n",
      "|      VTS|2010-01-27 18:15:00|2010-01-27 18:29:00|              1|         1.42|       -73.97962|       40.74378|           1|              null|        -73.98942|       40.756786|         Cre|        8.5|  1.0|    0.5|       2.0|         0.0|        12.0|\n",
      "|      VTS|2010-01-08 16:05:00|2010-01-08 16:13:00|              1|         0.84|      -74.000916|       40.75728|           1|              null|        -73.98977|        40.75741|         Cre|        5.7|  1.0|    0.5|       3.0|         0.0|        10.2|\n",
      "|      VTS|2010-01-09 02:07:00|2010-01-09 02:21:00|              1|         2.76|       -73.99117|      40.728073|           1|              null|        -73.97931|        40.74422|         CAS|       10.1|  0.5|    0.5|       0.0|         0.0|        11.1|\n",
      "|      VTS|2010-01-16 19:04:00|2010-01-16 19:22:00|              1|         1.52|       -73.98099|      40.737846|           1|              null|        -74.00326|       40.738716|         CAS|       10.5|  0.0|    0.5|       0.0|         0.0|        11.0|\n",
      "|      VTS|2010-01-08 17:15:00|2010-01-08 17:40:00|              1|         7.12|       -73.84417|       40.72135|           1|              null|        -73.93144|       40.670685|         CAS|       19.7|  1.0|    0.5|       0.0|         0.0|        21.2|\n",
      "|      DDS|2010-01-14 19:23:15|2010-01-14 19:44:25|              1|          1.4|      -73.955574|      40.772522|           1|              null|        -73.99112|       40.755463|         CAS|        9.7|  1.5|    0.0|       0.0|         0.0|        11.2|\n",
      "|      DDS|2010-01-06 22:34:49|2010-01-06 22:41:30|              1|          1.1|      -74.010254|       40.72055|           1|              null|        -74.00397|       40.715782|         CAS|        5.7|  0.5|   -1.0|       0.0|         0.0|         6.7|\n",
      "|      DDS|2010-01-20 15:52:26|2010-01-20 16:07:19|              1|          2.0|      -73.969536|       40.80055|           1|              null|        -73.94272|        40.84157|         CRE|        8.9|  0.0|    0.5|       1.1|         0.0|        10.5|\n",
      "|      DDS|2010-01-22 11:54:52|2010-01-22 11:59:38|              1|          1.1|       -73.99683|      40.716545|           1|              null|        -73.94854|        40.79509|         CAS|        4.9|  0.0|    0.5|       0.0|         0.0|         5.4|\n",
      "|      VTS|2010-01-26 16:59:00|2010-01-26 17:08:00|              2|         1.34|      -73.988815|      40.736656|           1|              null|        -73.98033|       40.723988|         CAS|        6.1|  1.0|    0.5|       0.0|         0.0|         7.6|\n",
      "|      VTS|2010-01-26 12:05:00|2010-01-26 12:16:00|              5|         1.64|       -73.99039|       40.74583|           1|              null|       -74.007805|        40.74515|         Cre|        7.7|  0.0|    0.5|       3.0|         0.0|        11.2|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+---+\n",
      "|single|double|key|\n",
      "+------+------+---+\n",
      "|     9|  null|  2|\n",
      "|    10|  null|  2|\n",
      "|     4|    16|  1|\n",
      "|     5|    25|  1|\n",
      "|     6|  null|  2|\n",
      "|     7|  null|  2|\n",
      "|     8|  null|  2|\n",
      "|     1|     1|  1|\n",
      "|     2|     4|  1|\n",
      "|     3|     9|  1|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_spark_read_csv_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "df_fire=check_spark_read_csv_time(csv_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "fire_output_path1=\"s3a://pengfei/diffusion/data_format/Fire_Department1.parquet\"\n",
    "# use gzip compression instead of snappy\n",
    "fire_output_path2=\"s3a://pengfei/diffusion/data_format/Fire_Department2.parquet\"\n",
    "# change default dictionary size to 512KB\n",
    "fire_output_path3=\"s3a://pengfei/diffusion/data_format/Fire_Department3.parquet\"\n",
    "# this parquet has default partition 8, so it has 8 parquet file\n",
    "fire_output_path0=\"s3a://pengfei/diffusion/data_format/Fire_Department.parquet\"\n",
    "\n",
    "# check_spark_write_time(df_fire,fire_output_path)\n",
    "\n",
    "# set the block size to 128MB, this does not work, because the number of parquet partition is set by the number of dataframe partition.\n",
    "# df_fire.write.option(\"parquet.block.size\",256 * 1024 * 1024).parquet(fire_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet block size\n",
    "\n",
    "When writing a parquet file to disk, we have to consider how the parquet file is stored physically. \n",
    "- In a block storage file system, if your partquet partition size is bigger than the block size,  your parquet partition will be splitted into blocks, it means the columns of row group are splitted into different blocks. Or even worse, one column is splitted into different blocks. So its recommended that the parquet block size should have the same block size of the block storage. The option **parquet.block.size** can help you to set the size of block\n",
    "- In an object storage file system, the parquet file partition will not be splitted, because object storage stores data as one single object. As a result, **parquet.block.size** is meaningless in this kind of situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# we can set the\n",
    "df_fire.coalesce(1).write \\\n",
    "# it only works for the block storage such as hdfs. \n",
    "\n",
    "# But for object storgage such as s3 or minio, we don't have the storage \n",
    ".option(\"parquet.block.size\",256 * 1024 * 1024) \\\n",
    ".option(\"parquet.page.size\",3*1024*1024) \\\n",
    ".option(\"parquet.dictionary.page.size\",512 * 1024) \\\n",
    ".option(\"parquet.enable.dictionary\", \"true\") \\\n",
    ".option(\"parquet.compression\",\"gzip\") \\\n",
    ".parquet(fire_output_path0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df0=check_spark_parquet_read_time(fire_output_path0)\n",
    "df1=check_spark_parquet_read_time(fire_output_path1)\n",
    "df2=check_spark_parquet_read_time(fire_output_path2)\n",
    "df3=check_spark_parquet_read_time(fire_output_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet configuration\n",
    "\n",
    "- parquet.block.size: It defines the default size of a parquet partition file. If you use hdfs to store these file, the Parquet file block size should be no larger than the HDFS block size for the file so that each Parquet block can be read from a single HDFS block (and therefore from a single datanode). It is common to set them to be the same, and indeed both defaults are for 128 MB block sizes. Note the default size does not mean that all parquet partition files will have the exact same size. But their size will be approximate to this value. For example, if the default size is 128MB, then one can have 127.66MB, one can have 126.98MB.\n",
    "\n",
    "- parquet.page.size: It defines the default size of a page in a column. A page is the smallest unit of storage in a Parquet file, so retrieving an arbitrary row requires that the page containing the row be decompressed and decoded. Thus, for single-row lookups, it is more efficient to have smaller pages, so there are fewer values to read through before reaching the target value.\n",
    "\n",
    "- parquet.dictionary.page.size: The maximum allow size in byte of a dictionary before falling back to plain encoding for a page\n",
    "\n",
    "- parquet.enable.dictionary: Enable dictionary encoding or not.\n",
    "\n",
    "- parquet.compress: choose the compression type.\n",
    "\n",
    "\n",
    "\n",
    "# Some tips:\n",
    "1. Dictionary encoding:\n",
    "Smaller files means there will be less I/O involved. Dictionary encoding will ensure that there is improvement in storage and accessing. For one column chunk there will be single dictionary. Most types are encoded using dictionary en‐ coding by default; however, a plain encoding will be used as a fallback if the dictionary becomes too large. The threshold size at which this happens is referred to as the dictionary page size and is the same as the page size by default. Please refer to parquet configuration section for more information. One can validate whether the file is dictionary encoded by using the parquet-tools.\n",
    "In order to perform better we need to decrease the row group size and increase the dictionary page size.\n",
    "2. Page Compression:\n",
    "The below default compression schemes while using the Parquet format.\n",
    "Spark uses snappy as default.\n",
    "Impala uses snappy as default.\n",
    "Hive uses deflate codec as default.\n",
    "Using snappy compression will reduce the size of the page and improve read time.\n",
    "Using Parquet format allows for better compression, as data is more homogeneous. The space savings are very noticeable at the scale of a Hadoop cluster. I/O will be reduced as we can efficiently scan only a subset of the columns while reading the data. Better compression also reduces the bandwidth required to read the input. As we store data of the same type in each column, we can use encoding better suited to the modern processors’ pipeline by making instruction branching more predictable. Parquet format is mainly used for WRITE ONCE READ MANY applications.\n",
    "I hope this blog helped you in understanding the parquet format and internal functionality. Happy Learning!!!\n",
    "\n",
    "\n",
    "# Spark parquet config set to false by default\n",
    "spark.sql.parquet.mergeSchema\n",
    "spark.sql.parquet.respectSummaryFiles\n",
    "spark.sql.parquet.binaryAsString\n",
    "spark.sql.parquet.int96TimestampConversion\n",
    "spark.sql.parquet.int64AsTimestampMillis\n",
    "spark.sql.parquet.writeLegacyFormat\n",
    "spark.sql.parquet.recordLevelFilter.enabled\n",
    "\n",
    "# Spark parquet config set to true by default\n",
    "\n",
    "spark.sql.parquet.int96AsTimestamp\n",
    "spark.sql.parquet.filterPushdown\n",
    "spark.sql.parquet.filterPushdown.date\n",
    "spark.sql.parquet.filterPushdown.timestamp\n",
    "spark.sql.parquet.filterPushdown.decimal\n",
    "spark.sql.parquet.filterPushdown.string.startsWith\n",
    "spark.sql.parquet.enableVectorizedReader\n",
    "\n",
    "# These properties need value and listing it with defaults-\n",
    "\n",
    "spark.sql.parquet.outputTimestampType = INT96\n",
    "spark.sql.parquet.compression.codec = snappy\n",
    "spark.sql.parquet.pushdown.inFilterThreshold = 10\n",
    "spark.sql.parquet.output.committer.class = org.apache.parquet.hadoop.ParquetOutputCommitter\n",
    "spark.sql.parquet.columnarReaderBatchSize = 4096\n",
    "\n",
    "Regarding parquet.enable.dictionary, it is not supported by Spark yet. But it can be set in sqlContext as -\n",
    "\n",
    "sqlContext.setConf(\"parquet.enable.dictionary\", \"false\")\n",
    "Default value is of this property is true in parquet. Therefore, it should be true when parquet code is called from Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
