{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark compatibility check\n",
    "In this section, we use spark to read parquet file that are generated by arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import time\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkReadArrow\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0915 14:22:37.084522    2169 request.go:655] Throttling request took 1.168941502s, request: GET:https://kubernetes.default/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s\n",
      "NAME                                                        READY   STATUS      RESTARTS   AGE\n",
      "argo-workflows-315671-server-7d5cf97d7-rft67                1/1     Running     0          2d7h\n",
      "argo-workflows-315671-workflow-controller-f9545ff99-nmzlj   1/1     Running     0          2d7h\n",
      "flume-test-agent-df8c5b944-j47lw                            1/1     Running     0          3d17h\n",
      "jupyter-107079-85887dc86c-4h46d                             1/1     Running     0          6h13m\n",
      "kafka-server-0                                              1/1     Running     0          3d19h\n",
      "kafka-server-1                                              1/1     Running     0          3d16h\n",
      "kafka-server-2                                              1/1     Running     0          4d17h\n",
      "kafka-server-zookeeper-0                                    1/1     Running     0          3d19h\n",
      "mlflow-752700-849d994c7c-ktcw2                              1/1     Running     0          47h\n",
      "mlflow-db-0                                                 1/1     Running     0          47h\n",
      "openfood-workflow-v1-6b7dx-2499703885                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-2894184417                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-3450937515                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-3747732804                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-6b7dx-987440259                        0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-d654v-101709803                        0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-1307372657                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-1413982577                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-2057086870                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-d654v-3251949423                       0/2     Completed   0          2d5h\n",
      "openfood-workflow-v1-fzsqf-1791550650                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-2177260728                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-4190006776                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-4240129049                       0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-fzsqf-671590606                        0/2     Completed   0          2d6h\n",
      "openfood-workflow-v1-rfhq6-1051279974                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-2311733954                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-3222782558                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-3944585314                       0/2     Completed   0          2d\n",
      "openfood-workflow-v1-rfhq6-727072442                        0/2     Completed   0          2d\n",
      "openfood-workflow-v1-v6n9j-1360484915                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-1705025569                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-2796457920                       0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-367709877                        0/2     Completed   0          2d1h\n",
      "openfood-workflow-v1-v6n9j-382658779                        0/2     Completed   0          2d1h\n",
      "postgres-1616502799-67f86f5bdf-2n5nl                        1/1     Running     0          2d5h\n",
      "sparkreadarrow-c8aa427be97763d6-exec-1                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-2                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-3                      1/1     Running     0          103m\n",
      "sparkreadarrow-c8aa427be97763d6-exec-4                      1/1     Running     0          103m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-1        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-2        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-3        1/1     Running     0          151m\n",
      "sparkstreamingcomputervision-6b813b7be94bd558-exec-4        1/1     Running     0          151m\n",
      "ubuntu-555591-684684bcc7-nw79c                              1/1     Running     0          3d17h\n",
      "vscode-120409-cb9cd6cd7-f57hb                               1/1     Running     0          3d5h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_input_path = \"s3a://pengfei/diffusion/data_format/arrow_netflix/\"\n",
    "output_path=\"s3a://pengfei/diffusion/data_format/spark_netflix/\"\n",
    "csv_example=\"s3a://pengfei/diffusion/data_format/Fire_Department_Calls_for_Service.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has 24058262 rows, 3 columns\n",
      "Spark read time spents: 0.8141007423400879 s\n",
      "data frame has 24058262 rows, 3 columns\n",
      "Spark read time spents: 0.8090753555297852 s\n"
     ]
    }
   ],
   "source": [
    "def check_spark_parquet_read_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.parquet(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "# read parquet generated by arrow    \n",
    "check_spark_parquet_read_time(parquet_input_path)\n",
    "\n",
    "# read parquet generated by spark\n",
    "check_spark_parquet_read_time(\"s3a://pengfei/diffusion/data_format/netflix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark write time spents: 26.110023498535156 s\n"
     ]
    }
   ],
   "source": [
    "def check_spark_write_time(df,path):\n",
    "    t1=time.time()\n",
    "    df.write.parquet(path)\n",
    "    t2=time.time()\n",
    "    print(f\"Spark write time spents: {t2 - t1} s\")\n",
    "    \n",
    "df=spark.read.parquet(parquet_input_path)\n",
    "check_spark_write_time(df,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|double|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     2|     4|\n",
      "|     3|     9|\n",
      "|     4|    16|\n",
      "|     5|    25|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|single|triple|\n",
      "+------+------+\n",
      "|     6|   216|\n",
      "|     7|   343|\n",
      "|     8|   512|\n",
      "|     9|   729|\n",
      "|    10|  1000|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-651d046d3c90>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0mdf2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema_output_path1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0mdf2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema_output_path2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1247\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1248\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_opts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1249\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1250\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1251\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlineSep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: path s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1 already exists."
     ]
    }
   ],
   "source": [
    "# merge schema example\n",
    "# In this example, we create two data frame, \n",
    "# df1, we have two columns: single, double.\n",
    "# df2, we have two columns: single, triple\n",
    "sc = spark.sparkContext\n",
    "schema_output_path1= \"s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=1\"\n",
    "schema_output_path2= \"s3a://pengfei/diffusion/data_format/merge_schema/test_table/key=2\"\n",
    "df1 = spark.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "df2.show()\n",
    "# then we write the two data frame in a partition folder, here we put key=1, key=2.\n",
    "df1.write.parquet(schema_output_path1)\n",
    "df2.write.parquet(schema_output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+---+\n",
      "|single|double|key|\n",
      "+------+------+---+\n",
      "|     9|  null|  2|\n",
      "|    10|  null|  2|\n",
      "|     4|    16|  1|\n",
      "|     5|    25|  1|\n",
      "|     6|  null|  2|\n",
      "|     7|  null|  2|\n",
      "|     8|  null|  2|\n",
      "|     1|     1|  1|\n",
      "|     2|     4|  1|\n",
      "|     3|     9|  1|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we read the parent folder that contains the partitioned folder. The partition key become a column name, we call it partitioned column\n",
    "parent_path=\"s3a://pengfei/diffusion/data_format/merge_schema/test_table\"\n",
    "# as the data frame in each partition folder has different schema, we need to set mergeSchema to true. Otherwise it will only use the schema\n",
    "# of the first parquet file which it reads. \n",
    "# set the below value to false to check the output data frame.\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(parent_path)\n",
    "mergedDF.printSchema()\n",
    "\n",
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_spark_read_csv_time(path):\n",
    "    t1=time.time()\n",
    "    df=spark.read.csv(path)\n",
    "    print(f\"data frame has {df.count()} rows, {len(df.columns)} columns\")\n",
    "    t2=time.time()\n",
    "    print(f\"Spark read time spents: {t2 - t1} s\")\n",
    "    return df\n",
    "\n",
    "df_fire=check_spark_read_csv_time(csv_example)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "fire_output_path1=\"s3a://pengfei/diffusion/data_format/Fire_Department1.parquet\"\n",
    "# use gzip compression instead of snappy\n",
    "fire_output_path2=\"s3a://pengfei/diffusion/data_format/Fire_Department2.parquet\"\n",
    "# change default dictionary size to 512KB\n",
    "fire_output_path3=\"s3a://pengfei/diffusion/data_format/Fire_Department3.parquet\"\n",
    "# this parquet has default partition 8, so it has 8 parquet file\n",
    "fire_output_path0=\"s3a://pengfei/diffusion/data_format/Fire_Department.parquet\"\n",
    "\n",
    "# check_spark_write_time(df_fire,fire_output_path)\n",
    "\n",
    "# set the block size to 128MB, this does not work, because the number of parquet partition is set by the number of dataframe partition.\n",
    "# df_fire.write.option(\"parquet.block.size\",256 * 1024 * 1024).parquet(fire_output_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# we can set the\n",
    "df_fire.coalesce(1).write \\\n",
    ".option(\"parquet.block.size\",256 * 1024 * 1024) \\\n",
    ".option(\"parquet.page.size\",3*1024*1024) \\\n",
    ".option(\"parquet.dictionary.page.size\",512 * 1024) \\\n",
    ".option(\"parquet.enable.dictionary\", \"true\") \\\n",
    ".option(\"parquet.compression\",\"gzip\") \\\n",
    ".parquet(fire_output_path0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet configuration\n",
    "\n",
    "- parquet.block.size: It defines the default size of a parquet partition file. If you use hdfs to store these file, the Parquet file block size should be no larger than the HDFS block size for the file so that each Parquet block can be read from a single HDFS block (and therefore from a single datanode). It is common to set them to be the same, and indeed both defaults are for 128 MB block sizes. Note the default size does not mean that all parquet partition files will have the exact same size. But their size will be approximate to this value. For example, if the default size is 128MB, then one can have 127.66MB, one can have 126.98MB.\n",
    "\n",
    "- parquet.page.size: It defines the default size of a page in a column. A page is the smallest unit of storage in a Parquet file, so retrieving an arbitrary row requires that the page containing the row be decompressed and decoded. Thus, for single-row lookups, it is more efficient to have smaller pages, so there are fewer values to read through before reaching the target value.\n",
    "\n",
    "- parquet.dictionary.page.size: The maximum allow size in byte of a dictionary before falling back to plain encoding for a page\n",
    "\n",
    "- parquet.enable.dictionary: Enable dictionary encoding or not.\n",
    "\n",
    "- parquet.compress: choose the compression type.\n",
    "\n",
    "\n",
    "\n",
    "# Some tips:\n",
    "1. Dictionary encoding:\n",
    "Smaller files means there will be less I/O involved. Dictionary encoding will ensure that there is improvement in storage and accessing. For one column chunk there will be single dictionary. Most types are encoded using dictionary en‐ coding by default; however, a plain encoding will be used as a fallback if the dictionary becomes too large. The threshold size at which this happens is referred to as the dictionary page size and is the same as the page size by default. Please refer to parquet configuration section for more information. One can validate whether the file is dictionary encoded by using the parquet-tools.\n",
    "In order to perform better we need to decrease the row group size and increase the dictionary page size.\n",
    "2. Page Compression:\n",
    "The below default compression schemes while using the Parquet format.\n",
    "Spark uses snappy as default.\n",
    "Impala uses snappy as default.\n",
    "Hive uses deflate codec as default.\n",
    "Using snappy compression will reduce the size of the page and improve read time.\n",
    "Using Parquet format allows for better compression, as data is more homogeneous. The space savings are very noticeable at the scale of a Hadoop cluster. I/O will be reduced as we can efficiently scan only a subset of the columns while reading the data. Better compression also reduces the bandwidth required to read the input. As we store data of the same type in each column, we can use encoding better suited to the modern processors’ pipeline by making instruction branching more predictable. Parquet format is mainly used for WRITE ONCE READ MANY applications.\n",
    "I hope this blog helped you in understanding the parquet format and internal functionality. Happy Learning!!!\n",
    "\n",
    "\n",
    "# Spark parquet config set to false by default\n",
    "spark.sql.parquet.mergeSchema\n",
    "spark.sql.parquet.respectSummaryFiles\n",
    "spark.sql.parquet.binaryAsString\n",
    "spark.sql.parquet.int96TimestampConversion\n",
    "spark.sql.parquet.int64AsTimestampMillis\n",
    "spark.sql.parquet.writeLegacyFormat\n",
    "spark.sql.parquet.recordLevelFilter.enabled\n",
    "\n",
    "# Spark parquet config set to true by default\n",
    "\n",
    "spark.sql.parquet.int96AsTimestamp\n",
    "spark.sql.parquet.filterPushdown\n",
    "spark.sql.parquet.filterPushdown.date\n",
    "spark.sql.parquet.filterPushdown.timestamp\n",
    "spark.sql.parquet.filterPushdown.decimal\n",
    "spark.sql.parquet.filterPushdown.string.startsWith\n",
    "spark.sql.parquet.enableVectorizedReader\n",
    "\n",
    "# These properties need value and listing it with defaults-\n",
    "\n",
    "spark.sql.parquet.outputTimestampType = INT96\n",
    "spark.sql.parquet.compression.codec = snappy\n",
    "spark.sql.parquet.pushdown.inFilterThreshold = 10\n",
    "spark.sql.parquet.output.committer.class = org.apache.parquet.hadoop.ParquetOutputCommitter\n",
    "spark.sql.parquet.columnarReaderBatchSize = 4096\n",
    "\n",
    "Regarding parquet.enable.dictionary, it is not supported by Spark yet. But it can be set in sqlContext as -\n",
    "\n",
    "sqlContext.setConf(\"parquet.enable.dictionary\", \"false\")\n",
    "Default value is of this property is true in parquet. Therefore, it should be true when parquet code is called from Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}