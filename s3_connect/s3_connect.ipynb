{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. S3 select api introduction\n",
    "When we use frameworks such as Spark, or Arrow to retrieve objects from s3. They always retrieve the whole entities of the objects. For example, if spark read a paruqet file of 10 GiB in s3, a total 10 GiB of data will be transfered from s3 to the spark cluster. Even though spark may just require some of the columns and rows to do the calculation. As a result, we retrived many useless data that increase the I/O of the operation.\n",
    "\n",
    "To avoid this, the S3 Select API allows us to retrieve a subset of data by using simple SQL expressions. The data filtering happens on the s3 server. And only the data needed by the application will be retrieved. This can improve drasticly the operation performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the S3 select api is designed for filtering columns and rows only. It's not designed for handling complex analytical queries and return results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limitation of S3 Select\n",
    "- It supports a maximum of 256 KB length of an SQL expression.\n",
    "- It supports a maximum of 1 MB length of a record in the input or result.\n",
    "- Few SQL clauses that are supported are **SELECT, FROM, WHERE, LIMIT**, etc.\n",
    "- It is not useful for complex analytical queries and joins.\n",
    "- Currently, only three object formats, **CSV, JSON, or Apache Parquet** are supported by S3 Select queries.\n",
    "- At a time, the select query can execute on a single file (object).\n",
    "- Minio only supports UTF-8 as the encoding type for select API.\n",
    "- **AWS S3** supports compression such as: NONE, GZIP, BZIP2. The default Value is NONE. **Minio** supports GZIP, Snappy, LZ4 for columnar compression of Parquet API. Whole object compression is not supported for Parquet objects.\n",
    "\n",
    "The first four limitations are normal, because S3 select is not designed for handling complex analytical queries.\n",
    "\n",
    "The last four limitations are quite annoying. Because parquet file are often partitioned into multiple blocks. And parquet supports much more compression algo that s3 select can't read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "session_token=os.getenv(\"AWS_SESSION_TOKEN\")\n",
    "endpoint=os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "\n",
    "# print(f\"key id: {key_id}\")\n",
    "# print(f\"key secret: {secret}\")\n",
    "# print(f\"session token: {session_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  donnees-insee\n",
      "  pengfei\n",
      "  projet-relevanc\n",
      "  projet-spark-lab\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=f'https://{endpoint}',\n",
    "    aws_access_key_id=key_id,\n",
    "    aws_secret_access_key=secret,\n",
    "    aws_session_token=session_token\n",
    ")\n",
    "\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file by using s3 select\n",
    "def fetch_csv(bucket:str,path:str,query:str):\n",
    "    \"\"\" Read csv file from s3 by using s3 select\n",
    "\n",
    "    Keyword arguments:\n",
    "    bucket -- the name of the bucket that the CSV file is in\n",
    "    path -- the path to the csv file \n",
    "    query -- the sql query specification that will be used to filter the data.\n",
    "    \"\"\"\n",
    "    response = s3_client.select_object_content(\n",
    "               Bucket=bucket,\n",
    "               Key=path,\n",
    "               ExpressionType='SQL',\n",
    "               Expression=query,\n",
    "               InputSerialization = {'CSV': {\"FileHeaderInfo\": \"Use\"}, 'CompressionType': 'NONE'},\n",
    "               OutputSerialization = {'CSV': {}},)\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_csv_content(response):\n",
    "    \"\"\" Print the content of the s3 select query\n",
    "    \n",
    "    Keyword arguments:\n",
    "    response -- the response of the boto client s3 call \n",
    "    \"\"\"\n",
    "    \n",
    "    for event in response['Payload']:\n",
    "        if 'Records' in event:\n",
    "            records = event['Records']['Payload'].decode('utf-8')\n",
    "            print(records)\n",
    "            \n",
    "    \n",
    "def print_query_stats(response):\n",
    "    \"\"\" Print the detailed stats of the s3 select query \n",
    "    \n",
    "    Keyword arguments:\n",
    "    response -- the response of the boto client s3 call \n",
    "    \"\"\"\n",
    "    \n",
    "    for event in response['Payload']:\n",
    "        if 'Stats' in event:\n",
    "            stats_details = event['Stats']['Details']\n",
    "            print(f\"Query bytes scanned : {stats_details['BytesScanned']} \")\n",
    "            print(f\"Query bytes processed: {stats_details['BytesProcessed']} \")\n",
    "            print(f\"Query bytes returned: {stats_details['BytesReturned']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv with s3 select.\n",
    "bucket=\"pengfei\"\n",
    "path=\"diffusion/data_format/netflix.csv\"\n",
    "q1=\"SELECT * FROM s3object s where s.\\\"rating\\\" = '5'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query bytes scanned : 495032025 \n",
      "Query bytes processed: 495032025 \n",
      "Query bytes returned: 113321890\n"
     ]
    }
   ],
   "source": [
    "resp1=fetch_csv(bucket,path,q1)        \n",
    "\n",
    "print_query_stats(resp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp1=fetch_csv(bucket,path,q1)        \n",
    "# print_csv_content(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query bytes scanned : 495032025 \n",
      "Query bytes processed: 495032025 \n",
      "Query bytes returned: 495032005\n"
     ]
    }
   ],
   "source": [
    "q2=\"SELECT * FROM s3object\"\n",
    "resp2=fetch_csv(bucket,path,q2)\n",
    "\n",
    "print_query_stats(resp2)\n",
    "# print_csv_content(resp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice the above two query scanned and processed the same amout of data. But in q1 we filter rows where \"rating\"='5', so it only returns 113321890B (113 MiB). In q2, we just select all the data, so it returns 495032005B (495 MiB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Use s3 select to read parquet \n",
    "\n",
    "In minio, the **s3 select Parquet is DISABLED** by default since hostile crafted input can easily crash the server.\n",
    "\n",
    "If you are in a controlled environment where it is safe to assume no hostile content can be uploaded to your cluster you can safely enable Parquet. To enable Parquet set the environment variable MINIO_API_SELECT_PARQUET=on.\n",
    "\n",
    "You can find the official minio s3 select doc [here](https://docs.min.io/docs/minio-select-api-quickstart-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InternalError) when calling the SelectObjectContent operation (reached max retries: 4): We encountered an internal error, please try again.: cause(parquet format parsing not enabled on server)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-d18a9c9a193c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mExpression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT * FROM s3object limit 5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mInputSerialization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Parquet'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CompressionType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'NONE'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mOutputSerialization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'CSV'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InternalError) when calling the SelectObjectContent operation (reached max retries: 4): We encountered an internal error, please try again.: cause(parquet format parsing not enabled on server)"
     ]
    }
   ],
   "source": [
    "# read parquet with s3 select.\n",
    "data_path=\"diffusion/data_format/sf_fire/parquet/arrow_sf_fire_none/f402f99cb6d9459696314909b6f6e0a3.parquet\"\n",
    "\n",
    "resp = s3_client.select_object_content(\n",
    "    Bucket='pengfei',\n",
    "    Key=data_path,\n",
    "    ExpressionType='SQL',\n",
    "    Expression=\"SELECT * FROM s3object limit 5\",\n",
    "    InputSerialization = {'Parquet': {}, 'CompressionType': 'NONE'},\n",
    "    OutputSerialization = {'CSV': {}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
